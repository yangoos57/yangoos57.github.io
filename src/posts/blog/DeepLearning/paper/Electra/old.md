---
title: "ELECTRA ëª¨ë¸ êµ¬í˜„ ë° Domain Adaptation ë°©ë²• ì •ë¦¬"
category: "DeepLearning"
date: "2020-12-22"
thumbnail: "./img/electra.png"
desc: "pytorchë¥¼ í™œìš©í•´ ELECTRA ë…¼ë¬¸ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©° Generatorì™€ Descriminator ê°„ ì—°ê²° ë°©ë²• ë° Replace Token Detection(RTD)ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤.
Huggingfaceì˜ trainerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•˜ê³ , ì´ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì„ ì œì‘í•´ ELECTRA ë¿ë§Œ ì•„ë‹ˆë¼ Huggingface ì‚¬ìš©ë²•ì„ ì†ì‰½ê²Œ ìµí ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤. ì§ì ‘ Domain Adapatationì„ ê²½í—˜í•˜ë©° ELECTRA í•™ìŠµ ë°©ë²• ë° ë°ì´í„° íë¦„ì— ëŒ€í•´ ì´í•´í•  ìˆ˜ ìˆë‹¤."
---

### ì™œ ELECTRAì¸ê°€?

- ELECTRAëŠ” Masked Language Model(MLM)ì˜ ë¹„íš¨ìœ¨ì ì¸ í•™ìŠµ ë°©ë²•ì— ìƒˆë¡œìš´ ëŒ€ì•ˆì„ ì œì‹œí•˜ëŠ” ëª¨ë¸ì„. ì§€ê¸ˆê» ì–¸ì–´ ëª¨ë¸ì€ í†µê³„ ê¸°ë°˜ ëª¨ë¸, Vector Space ë‚´ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ë°°ì¹˜í•˜ëŠ” ëª¨ë¸, Maskë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì´ ë“±ì¥í•˜ë©° ë³€í™”ë¥¼ ë¶ˆëŸ¬ì™”ìŒ.

- ELECTRAê°€ ì œì‹œí•˜ëŠ” replaced token detection(RTD) í•™ìŠµ ë°©ë²•ì€ ë™ì¼ í™˜ê²½, ë™ì¼ ì‹œê°„ ëŒ€ë¹„ MLM ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¥í•¨. ë™ì¼í•œ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ì„œ RTDê°€ MLMì— ë¹„í•´ ë” ì ì€ ì»´í“¨íŒ… ì‹œê°„ì„ ì†Œëª¨í•œë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ íš¨ìœ¨ì ì¸ í•™ìŠµ ë°©ë²•ì´ë¼ í•  ìˆ˜ ìˆìŒ.

- ELECTRAëŠ” BERTë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì‹œí•˜ëŠ” ëª¨ë¸ì´ë¯€ë¡œ BERT êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ìˆë‹¤ë©´ ì–´ë µì§€ ì•Šê²Œ ë…¼ë¬¸ì„ ì´í•´í•  ìˆ˜ ìˆìŒ.

<br/>
<br/>

### ELECTRA íŠ¹ì§•

- MLM ëª¨ë¸ì´ ë¹„íš¨ìœ¨ì ì¸ ì´ìœ ëŠ” ë¬¸ì¥ì˜ 15%ë§Œì„ í•™ìŠµì— í™œìš©í•˜ê¸° ë•Œë¬¸ì„. ëª¨ë¸ì€ [Mask]ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ë°, ë¬¸ì¥ì˜ ì•½ 15% í† í°ì´ ì„ì˜ë¡œ ì„ íƒ ëœ ë’¤ ì „í™˜ë˜ë¯€ë¡œ [Mask] ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ë¬¸ì¥ì€ í•™ìŠµì„ í•˜ì§€ ì•Šê²Œ ë¨.

- ì´ëŸ¬í•œ ë¹„íš¨ìœ¨ì„ ê°œì„ í•˜ê³ ì ëª¨ë¸ì´ ë¬¸ì¥ ë‚´ í† í°ì„ ì „ë¶€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ RTDì„. RTDëŠ” ì•„ë˜ì™€ ì ˆì°¨ë¡œ ì§„í–‰ë¨.
  - Generator ëª¨ë¸ì—ì„œ ë¬¸ì¥ í† í° ì¤‘ ì•½ 15%ë¥¼ ë°”ê¿” ê°€ì§œ ë¬¸ì¥ì„ ë§Œë“¬. GeneratorëŠ” ê¸°ì¡´ MLM í•™ìŠµ ë°©ë²•ëŒ€ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•¨.
  - DiscriminatorëŠ” ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ ì§„ì§œ í† í°ì¸ì§€, ê°€ì§œ í† í°ì¸ì§€ êµ¬ë³„í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµì„ ìˆ˜í–‰í•¨.
- í•™ìŠµì´ ì™„ë£Œë˜ë©´ GeneratorëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³  RTDë¡œ í•™ìŠµëœ Discriminatorë§Œì„ í™œìš©í•¨.

<br/>
<br/>

### Domain Adaptationì„ ìœ„í•œ ELECTRA í•™ìŠµêµ¬ì¡° ì„¤ê³„

> Domain Adaptationì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ê²½ìš° [[NLP] Further Pre-training ë° Fine-tuning ì •ë¦¬
> ](https://yangoos57.github.io/blog/DeepLearning/paper/Finetuning/Finetuning)ë¥¼ ì°¸ê³ 

- í•™ìŠµ êµ¬ì¡°ëŠ” [lucidrainsì˜ electra-pytorch](https://github.com/lucidrains/electra-pytorch) ì½”ë“œë¥¼ í™œìš©í–ˆìœ¼ë©°, Huggingfaceì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¼ë¶€ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì˜€ìŒ.
- Base ëª¨ë¸ë¡œ monologgë‹˜ì˜ `koelectra-v-base`ëª¨ë¸ì„ í™œìš©í–ˆìŒ.
- ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ì— ëŒ€í•œ `íŠœí† ë¦¬ì–¼`ì€ [Electra_for_Domain_Adaptation](https://github.com/yangoos57/Electra_for_fine_tuning)ë¥¼ ì°¸ê³ 
- ëª¨ë“  ì½”ë“œëŠ” `ğŸ¤— Transformers`ì™€ `pytorch` ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŒ.

<br/>

### 1. Huggingface Transformersë¡œ Pre-trained Model ë¶ˆëŸ¬ì˜¤ê¸°

- Discriminatorë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ëª¨ë“ˆì€ `ElectraForPreTraining`ì„ ì‚¬ìš©í•´ì•¼í•˜ê³ , Generatorë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ëª¨ë“ˆì€ `ElectraForMaskedLM` ì„ ì‚¬ìš©í•´ì•¼ í•¨.

- `ElectraForPreTraining`ëŠ” Discriminatorê°€ í•™ìŠµì— í•„ìš”í•œ tokenì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë³„í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ê³ , `ElectraForMaskedLM`ëŠ” Generatorê°€ ê°€ì§œ ë¬¸ì¥ì„ ìƒì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í† í°ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì„.

- ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„  HuggingFaceì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì´í•´í•´ì•¼í•¨.

```python

from transformers import ElectraForPreTraining, ElectraTokenizer, ElectraForMaskedLM

tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-generator")

generator = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')

discriminator = ElectraForPreTraining.from_pretrained("monologg/koelectra-base-v3-discriminator")

```

- ElectraForMaskedLM ëª¨ë¸ì˜ ì½”ë“œê°€ êµ¬í˜„ëœ ê²ƒì„ ë³´ë©´ `__init__`ì—ì„œ electra ëª¨ë¸ì„ Transformersì˜ ElectraModel ëª¨ë“ˆì—ì„œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ.

- ElectraModelì˜ Outputì€ Encoder ë§ˆì§€ë§‰ë‹¨ì˜ output('last-hidden-state')ì¸ë°, ì´ ê°’ì´ generator_predictions layerì™€ generator_lm_head layerë¡œ ì´ì–´ì ¸ ë“¤ì–´ê°€ `ElectraForMaskedLM` outputì„ ì‚°ì¶œí•˜ëŠ” êµ¬ì¡°ì„ì„ í™•ì¸ í•  ìˆ˜ ìˆìŒ.

```python

# Huggingface Transformers ë‚´ë¶€ ElectraForMaskedLM ì½”ë“œ

class ElectraForMaskedLM(ElectraPreTrainedModel):
    _keys_to_ignore_on_load_missing = ["generator_lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)

        # ElectraForMaskedLM ë‚´ë¶€ì—ì„œ ElectraModel ëª¨ë“ˆì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•¨.
        self.electra = ElectraModel(config)

        # [Mask] í† í°ì— ì í•©í•œ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” generator predictorê°€ í¬í•¨ë˜ì–´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ.
        self.generator_predictions = ElectraGeneratorPredictions(config)

        self.generator_lm_head = nn.Linear(config.embedding_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(...) :

            # electraë¥¼ í†µí•´ ë§ˆì§€ë§‰ encoderì˜ output(=last_hidden_state)ë¥¼ ë°›ìŒ.
            generator_hidden_states = self.electra(
                input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                position_ids=position_ids,
                head_mask=head_mask,
                inputs_embeds=inputs_embeds,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )
            # last_hidden_statesë¥¼ prediction scoresì— input ë°ì´í„°ë¡œ í™œìš©í•¨.
            generator_sequence_output = generator_hidden_states[0]

            # last_hidden_statesë¥¼ MLM prediction layerì˜ input ë°ì´í„°ë¡œ í™œìš©í•¨.
            prediction_scores = self.generator_predictions(generator_sequence_output)
            prediction_scores = self.generator_lm_head(prediction_scores)

    return MaskedLMOutput(
                loss=loss,
                # Prediction_scoreì„ ë¦¬í„´
                # logitsì˜ shapeì€ (batch_size, src_token_len, vocab_size)
                logits=prediction_scores,
                hidden_states=generator_hidden_states.hidden_states,
                attentions=generator_hidden_states.attentions,
            )

```

- `ElectraForPreTraining` ë„ ë§ˆì°¬ê°€ì§€ë¡œ `ElectraModel` ëª¨ë“ˆì„ ë² ì´ìŠ¤ë¡œ í•˜ê³  ë¬¸ì¥ ë‚´ ê°œë³„ tokenì´ ì§„ì§œ tokenì¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” classification layerê°€ ì—°ê²°ë˜ì–´ ìˆìŒ. ê°œë³„ í† í°ì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ 0ê³¼ 1ë¡œ íŒë‹¨í•¨. outputì´ 1ì¸ ê²½ìš° ëª¨ë¸ì´ ê°€ì§œ í† í°ìœ¼ë¡œ íŒë³„í•¨ì„ ì˜ë¯¸í•¨. ê°œë³„ tokenì— ëŒ€í•œ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ë¯€ë¡œ shapeëŠ” (batch_size, src_token_len)ì„.

```python

# Huggingface Transformers ë‚´ë¶€ ElectraForPreTraining ì½”ë“œ

class ElectraForPreTraining(ElectraPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        # ElectraForPreTraining ë‚´ë¶€ì—ì„œ ElectraModel ëª¨ë“ˆì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•¨.
        self.electra = ElectraModel(config)

        # Tokenì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” Precdiction ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´.
        self.discriminator_predictions = ElectraDiscriminatorPredictions(config)

    ....


    def forward(...)
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # electraë¥¼ í†µí•´ ë§ˆì§€ë§‰ encoderì˜ output(=last_hidden_state)ë¥¼ ë°›ìŒ.
        discriminator_hidden_states = self.electra(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # last_hidden_statesë¥¼ classification layerì˜ input ë°ì´í„°ë¡œ í™œìš©í•¨.
        discriminator_sequence_output = discriminator_hidden_states[0]

        logits = self.discriminator_predictions(discriminator_sequence_output)

        return ElectraForPreTrainingOutput(
            loss=loss,
            # logitsì„ ë¦¬í„´
            # logitsì˜ shapeì€ (batch_size, src_token_len)
            logits=logits,
            hidden_states=discriminator_hidden_states.hidden_states,
            attentions=discriminator_hidden_states.attentions,
        )
```

<br>

> **í•™ìŠµì„ ì™„ë£Œí•œ ê²½ìš°ì—ëŠ” `ElectraForPreTraining` ë‚´ë¶€ì— ìˆëŠ” ElectraModelë¥¼ ì¶”ì¶œí•´ finetuningì— í™œìš©í•¨. í•™ìŠµ ì™„ë£Œí•œ ëª¨ë¸ì—ì„œ ElectraModelì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ìŒ.**
>
> ```python
> discriminator = ElectraForPreTraining.from_pretrained('...')
>
> # Electra Model ì¶”ì¶œ
> trained_electra = discriminator.electra
>
> # ì¶”ì¶œëœ electra ëª¨ë¸ì€ Encoder ì¶œë ¥ ëë‹¨(=last_hidden_states)ë¥¼ outputìœ¼ë¡œ ì œê³µí•¨.
> # ElectraModelì„ í™œìš©í•´ Finetuning ìˆ˜í–‰
> ```

<br/>

### 2. í•™ìŠµ ëª¨ë¸ ì„¤ê³„í•˜ê¸°

- Transformersì—ì„œ ë¶ˆëŸ¬ì˜¨ Generatorì™€ Discriminatorë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œëŠ” í•™ìŠµìš© ëª¨ë¸ ì„¤ê³„ê°€ í•„ìˆ˜ì„.
  > í•´ë‹¹ ëª¨ë¸ì€ Domain Adaptation ë˜ëŠ” pre-traing from scratchë¥¼ ìœ„í•œ ëª¨ë¸ì´ë©°, Fine tuningì„ ìˆ˜í–‰í•  ê²½ìš° ì•„ë˜ì˜ í•™ìŠµ ëª¨ë¸ ì„¤ê³„ ì—†ì´ discriminatorë§Œ í™œìš©í•˜ë©´ ë¨.
- ì•„ë˜ì˜ í•™ìŠµ ëª¨ë¸ì€ ì•„ë˜ ë…¼ë¬¸ì˜ êµ¬ì¡°ë¥¼ êµ¬í˜„í•œ ê²ƒì„. electraì˜ í•™ìŠµ ë°©ì‹ì€ ì„¸ ë‹¨ê³„ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìŒ.

<img src='img/electra_sm.png'/>

- 1ë‹¨ê³„ : input data masking
- 2ë‹¨ê³„ : Generator í•™ìŠµ ë° fake sentence ìƒì„±
- 3ë‹¨ê³„ : Discriminator í•™ìŠµ

```python
import math
from functools import reduce
from collections import namedtuple

import torch
from torch import nn
import torch.nn.functional as F

# constants

Results = namedtuple(
    "Results",
    [
        "loss",
        "mlm_loss",
        "disc_loss",
        "gen_acc",
        "disc_acc",
        "disc_labels",
        "disc_predictions",
    ],
)

# ëª¨ë¸ ë‚´ë¶€ì—ì„œ í™œìš©ë˜ëŠ” í•¨ìˆ˜ ì •ì˜

def log(t, eps=1e-9):
    return torch.log(t + eps)

def gumbel_noise(t):
    noise = torch.zeros_like(t).uniform_(0, 1)
    return -log(-log(noise))

def gumbel_sample(t, temperature=1.0):
    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)

def prob_mask_like(t, prob):
    return torch.zeros_like(t).float().uniform_(0, 1) < prob

def mask_with_tokens(t, token_ids):
    init_no_mask = torch.full_like(t, False, dtype=torch.bool)
    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)
    return mask

def get_mask_subset_with_prob(mask, prob):
    batch, seq_len, device = *mask.shape, mask.device
    max_masked = math.ceil(prob * seq_len)

    num_tokens = mask.sum(dim=-1, keepdim=True)
    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()
    mask_excess = mask_excess[:, :max_masked]

    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)
    _, sampled_indices = rand.topk(max_masked, dim=-1)
    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)

    new_mask = torch.zeros((batch, seq_len + 1), device=device)
    new_mask.scatter_(-1, sampled_indices, 1)
    return new_mask[:, 1:].bool()

# main electra class

class Electra(nn.Module):
    def __init__(
        self,
        generator,
        discriminator,
        tokenizer,
        *,
        num_tokens=35000,
        mask_prob=0.15,
        replace_prob=0.85,
        mask_token_id=4,
        pad_token_id=0,
        mask_ignore_token_ids=[2, 3],
        disc_weight=50.0,
        gen_weight=1.0,
        temperature=1.0,
    ):
        super().__init__()

        """
        num_tokens: ëª¨ë¸ vocab_size
        mask_prob: í† í° ì¤‘ [MASK] í† í°ìœ¼ë¡œ ëŒ€ì²´ë˜ëŠ” ë¹„ìœ¨
        replace_prop:  í† í° ì¤‘ [MASK] í† í°ìœ¼ë¡œ ëŒ€ì²´ë˜ëŠ” ë¹„ìœ¨(?????)
        mask_token_i: [MASK] Token id
        pad_token_i: [PAD] Token id
        mask_ignore_token_id: [CLS],[SEP] Token id
        disc_weigh: discriminator lossì˜ Weight ì¡°ì •ì„ ìœ„í•œ ê°’
        gen_weigh: generator lossì˜ Weight ì¡°ì •ì„ ìœ„í•œ ê°’
        temperature: gumbel_distributionì— í™œìš©ë˜ëŠ” arg, ê°’ì´ ë†’ì„ìˆ˜ë¡ ëª¨ì§‘ë‹¨ ë¶„í¬ì™€ ìœ ì‚¬í•œ sampling ìˆ˜í–‰
        """

        self.generator = generator
        self.discriminator = discriminator
        self.tokenizer = tokenizer

        # mlm related probabilities
        self.mask_prob = mask_prob
        self.replace_prob = replace_prob

        self.num_tokens = num_tokens

        # token ids
        self.pad_token_id = pad_token_id
        self.mask_token_id = mask_token_id
        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])

        # sampling temperature
        self.temperature = temperature

        # loss weights
        self.disc_weight = disc_weight
        self.gen_weight = gen_weight

    def forward(self, input_ids, **kwargs):

        input = input_ids["input_ids"]

        # ------ 1ë‹¨ê³„ Input Data Masking --------#

        """
        - GeneratorëŠ” Bertì™€ êµ¬ì¡°ë„ ë™ì¼í•˜ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ë„ ë™ì¼í•¨.

        - Generator í•™ìŠµì„ ìœ„í•´ì„  [Masked] í† í°ì´ í•„ìš”í•˜ë¯€ë¡œ input dataë¥¼ Maskingí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•¨.

        """

        replace_prob = prob_mask_like(input, self.replace_prob)

        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])
        # also do not include these special tokens in the tokens chosen at random
        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)
        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)

        # get mask indices
        mask_indices = torch.nonzero(mask, as_tuple=True)

        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)
        masked_input = input.clone().detach()

        # set inverse of mask to padding tokens for labels
        gen_labels = input.masked_fill(~mask, self.pad_token_id)

        # clone the mask, for potential modification if random tokens are involved
        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens
        masking_mask = mask.clone()

        # [mask] input
        masked_input = masked_input.masked_fill(
            masking_mask * replace_prob, self.mask_token_id
        )

        # ------ 2ë‹¨ê³„ Masking ëœ ë¬¸ì¥ì„ Generatorê°€ í•™ìŠµí•˜ê³  ê°€ì§œ Tokenì„ ìƒì„± --------#

        """
        - Generatorë¥¼ í•™ìŠµí•˜ì—¬ MLM_loss ê³„ì‚°(combined_loss ê³„ì‚°ì— í™œìš©)
        - Generatorì—ì„œ ì˜ˆì¸¡í•œ ë¬¸ì¥ì„ Discriminator í•™ìŠµì— í™œìš©
        - ex) ì›ë³¸ ë¬¸ì¥ : ~~~
              ë§ˆìŠ¤í‚¹ ë¬¸ì¥ :
              ê°€ì§œ ë¬¸ì¥ :
        """

        # get generator output and get mlm loss(ìˆ˜ì •)
        logits = self.generator(masked_input, **kwargs).logits

        mlm_loss = F.cross_entropy(
            logits.transpose(1, 2), gen_labels, ignore_index=self.pad_token_id
        )

        # use mask from before to select logits that need sampling
        sample_logits = logits[mask_indices]

        # sample
        sampled = gumbel_sample(sample_logits, temperature=self.temperature)

        # scatter the sampled values back to the input
        disc_input = input.clone()
        disc_input[mask_indices] = sampled.detach()

        # generate discriminator labels, with replaced as True and original as False
        disc_labels = (input != disc_input).float().detach()

        # ------ 3ë‹¨ê³„ Tokenì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ Discriminatorê°€ ì˜ˆì¸¡í•˜ê³  ì´ë¥¼ í•™ìŠµ --------#

        """
        - ê°€ì§œ ë¬¸ì¥ì„ í•™ìŠµí•´ ê°œë³„ í† í°ì— ëŒ€í•´ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë‹¨
        - ì§„ì§œ tokenì´ë¼ íŒë‹¨í•˜ë©´ 0, ê°€ì§œ í† í°ì´ë¼ íŒë‹¨í•˜ë©´ 1ì„ ë¶€ì—¬
        - ì •ë‹µê³¼ ë¹„êµí•´ disc_lossë¥¼ ê³„ì‚°(combined_loss ê³„ì‚°ì— í™œìš©)
        - combined_loss : í•™ìŠµì˜ ìµœì¢… lossì„. ëª¨ë¸ì€ combined_lossì˜ ìµœì†Ÿê°’ì„ ì–»ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ì§„í–‰
        """

        # get discriminator predictions of replaced / original
        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)

        # get discriminator output and binary cross entropy loss
        disc_logits = self.discriminator(disc_input, **kwargs).logits
        disc_logits_reshape = disc_logits.reshape_as(disc_labels)

        disc_loss = F.binary_cross_entropy_with_logits(
            disc_logits_reshape[non_padded_indices], disc_labels[non_padded_indices]
        )

        # combined loss ê³„ì‚°
        # disc_weightì„ 50ìœ¼ë¡œ ì£¼ëŠ” ì´ìœ ëŠ” discriminatorì˜ taskê°€ ë³µì¡í•˜ì§€ ì•Šê¸° ë–„ë¬¸ì„.
        # mlm lossì˜ ê²½ìš° vocab_size(=35000) ë§Œí¼ì˜ loos ê³„ì‚°ì„ ìˆ˜í–‰í•˜ì§€ë§Œ
        # disc_lossì˜ ê²½ìš° src_token_len ë§Œí¼ì˜ loss ê³„ì‚°ì„ ìˆ˜í–‰í•œë§Œí¼
        # loss ê°’ì— í° ì°¨ì´ê°€ ë°œìƒí•¨. disc_weightì€ ì´ë¥¼ ë³´ì™„í•˜ëŠ” weightì„.
        combined_loss = (self.gen_weight * mlm_loss + self.disc_weight * disc_loss,)

        # ------ ëª¨ë¸ ì„±ëŠ¥ ë° í•™ìŠµ ê³¼ì •ì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§€í‘œ(Metrics) ì„¤ê³„ --------#

        with torch.no_grad():
            # gen mask ì˜ˆì¸¡
            gen_predictions = torch.argmax(logits, dim=-1)

            # fake token ì§„ìœ„ ì˜ˆì¸¡
            disc_predictions = torch.round(
                (torch.sign(disc_logits_reshape) + 1.0) * 0.5
            )
            # generator_accuracy
            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()
            # discriminator_accuracy
            disc_acc = (
                0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean()
                + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()
            )


        return Results(
            combined_loss,
            mlm_loss,
            disc_loss,
            gen_acc,
            disc_acc,
            disc_labels,
            disc_predictions,
        )
```

<br/>

### 3. Huggingface Datasetsìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

- ì´ ê¸€ì—ì„œëŠ” Hugging faceì˜ Trainer APIë¥¼ í™œìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•  ì˜ˆì •ì„. Trainer APIë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ë°ì´í„°ë¥¼ Huggingfaceì˜ Datasetsìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ê°•ë ¥í•˜ê²Œ ê¶Œí•¨.
- pytorchì˜ Datasetì„ í™œìš©í•  ìˆ˜ ìˆê¸´ í•˜ì§€ë§Œ Trainerì™€ í•¨ê»˜ ì‚¬ìš©í•˜ê¸°ì—ëŠ” ì›ì¸ì„ ì°¾ê¸° í˜ë“  ì—ëŸ¬ê°€ ë§ì•„ ë””ë²„ê¹…ì— ì–´ë ¤ì›€ì´ ìˆìŒ.

```python
from datasets import load_dataset

# local fileì„ ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ì„ë• 'í™•ì¥ìëª…', 'ê²½ë¡œ'ë¥¼ ì ìœ¼ë©´ ë¨
train = load_dataset('csv',data_files='data/book_train_128.csv')
validation = load_dataset('csv',data_files='data/book_validation_128.csv')

# tokenizing ë°©ë²• ì •ì˜
def tokenize_function(examples):
    return tokenizer(examples['sen'], max_length=128, padding=True, truncation=True)

# datasetsì˜ map ë§¤ì„œë“œë¥¼ í™œìš©í•´ ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
train_data_set = train['train'].map(tokenize_function,batch_size=True)
validation_data_set = validation['train'].map(tokenize_function,batch_size=True)
```

- datasets ê¸°ë³¸ ë§¤ì„œë“œ ì†Œê°œ

```python
train = load_dataset('csv',data_files='data/book_train_128.csv')

train

>>> DatasetDict({
    train: Dataset({
        features: ['Unnamed: 0', 'sen'],
        num_rows: 175900
    })
})

#----------

# column ì œê±°
train = train.remove_columns('Unnamed: 0')

>>> DatasetDict({
    train: Dataset({
        features: ['sen'],
        num_rows: 175900
    })
})

#----------

# train ë°ì´í„°ì…‹ìœ¼ë¡œ ì´ë™
train_data_set = train['train']

>>> Dataset({
    features: ['Unnamed: 0', 'sen'],
    num_rows: 175900
})

#----------

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_data_set[0]

>> {'sen': 'ì´ ì±…ì˜ íŠ¹ì§•ã†ì½”ë”©ì˜ ê¸°ì´ˆ ê¸°ì´ˆìˆ˜í•™ ë…¼ë¦¬ì˜ ... ê¸°ì´ˆìˆ˜í•™'}

#----------

# ë°ì´í„° ì¶”ì¶œ
train_data_set[0]['sen']

>>> 'ì´ ì±…ì˜ íŠ¹ì§•ã†ì½”ë”©ì˜ ê¸°ì´ˆ ê¸°ì´ˆìˆ˜í•™ ë…¼ë¦¬ì˜ ... ê¸°ì´ˆìˆ˜í•™'

#----------

# type í™•ì¸
train_data_set.feature

>>> {'sen': Value(dtype='string', id=None)}

#----------

# ì €ì¥
train_data_set.to_csv('')

```

<br/>

### 4. Transformers Trainer APIë¡œ ëª¨ë¸ í•™ìŠµí•˜ê¸°

#### â– í›ˆë ¨ ì˜µì…˜ ì„¤ì •(ì„ íƒì‚¬í•­)

í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ëª¨ë“  Argumentë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŒ. ì´ì¤‘ `logging_steps` ì— ëŒ€í•´ì„œë§Œ ì„¤ëª…í•˜ê² ìŒ. stepì€ 1íšŒ batch ì§„í–‰ì„ ì˜ë¯¸í•¨. logging_steps = 2ëŠ” 2íšŒì˜ stepì´ ëë‚˜ë©´ logë¥¼ print í•˜ë¼ëŠ” ëª…ë ¹ì–´ì„. logì— ëŒ€í•œ ë‚´ìš©ì€ callback í•¨ìˆ˜ë¥¼ ì„¤ëª…í•˜ë©° ë‹¤ë£¨ê² ìŒ.

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="test_trainer",
    per_device_eval_batch_size=8,
    per_device_train_batch_size=8,
    logging_steps=2,
    num_train_epochs=2,
    evaluation_strategy='steps'
)
```

<br/>

#### â– Callback ì„¤ì •(ì„ íƒì‚¬í•­)

> ì•„ë˜ì˜ ë‚´ìš©ê³¼ ê³µì‹ í™ˆí˜ì´ì§€ì˜ [Callback í˜ì´ì§€](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.CometCallback)ë¥¼ í•¨ê»˜ ì½ìœ¼ë©´ callbackì— ëŒ€í•´ ë¹ ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìˆìŒ

- callbackì€ í›ˆë ¨ ê³¼ì • ì¤‘ Trainer APIê°€ ì¶”ê°€ë¡œ ìˆ˜í–‰í•´ì•¼í•˜ëŠ” ë‚´ìš©ì„ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ì„.
- ì˜ˆë¡œë“¤ì–´ stepì„ ì‹œì‘í• ë•Œ ë§ˆë‹¤ ëª‡ë²ˆì§¸ stepì¸ì§€ printí•˜ê³  ì‹¶ì„ë•Œ í™œìš©í•  ìˆ˜ ìˆìŒ.
- callback classë¥¼ ì •ì˜ í•œ ë’¤ callbackì´ í•„ìš”í•œ ìˆœì„œë¥¼ í•¨ìˆ˜ë¡œ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•¨.
  - callbackì´ ê°€ëŠ¥í•œ ìˆœì„œëŠ” `on_init_end`, `on_train_begin`, `on_train_end`, `on_epoch_begin`, `on_epoch_end`, `on_step_begin`, `on_substep_end`, `on_step_end`, `on_evaluate`, `on_save`, `on_log`, `on_prediction_step` ì´ ìˆìŒ
- callback ë‚´ë¶€ í•¨ìˆ˜ëŠ” `arg`, `state`, `control`, `logs`, `**kwargs`ë¡œ ëª¨ë‘ ë™ì¼í•¨.

  - argëŠ” í›ˆë ¨ ì˜µì…˜ìœ¼ë¡œ ì„¤ì •í•œ ê°’ì„ ë¶ˆëŸ¬ì˜´.
  - stateëŠ” í˜„ì¬ step, epoch ë“± ì§„í–‰ ìƒíƒœì— ëŒ€í•œ ê°’ì„ ë¶ˆëŸ¬ì˜´
  - controlì€ í›ˆë ¨ ê³¼ì •ì„ í†µì œí•˜ëŠ” ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜´
  - logsëŠ” loss, lr, epoch ë“± ê¸°ë³¸ì ì¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜´
    ```python
    # logs output
    {'loss': 1.5284, 'learning_rate': 4.995452064762598e-05, 'epoch': 0.0}
    ```
  - `**kwargs` ëŠ” model, tokenizer, optimizer, dataloader ë“±ì„ ë¶ˆëŸ¬ ì˜¬ ìˆ˜ ìˆìŒ.

    ```python
    ### trainer_callback.py ì°¸ê³ 

    class CallbackHandler(TrainerCallback):
        """Internal class that just calls the list of callbacks in order."""

        def __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler):
            self.callbacks = []
            for cb in callbacks:
                self.add_callback(cb)
            # kwargsë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë“¤
            self.model = model
            self.tokenizer = tokenizer
            self.optimizer = optimizer
            self.lr_scheduler = lr_scheduler
            self.train_dataloader = None
            self.eval_dataloader = None
    ```

- ë¯¸ë¦¬ ì •ì˜ ëœ Callbackì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ.
  - Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í•´ë‹¹ callbackëª…ì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©
  - `ProgressCallback` ì€ on_train_begin ë‹¨ê³„ì—ì„œ ì§„í–‰ ìƒíƒœë°”ë¥¼ callbackí•˜ë„ë¡ ì„¤ì •
  - `PrinterCallback` ì€ on_log ìˆœì„œì—ì„œ logs ë‚´ìš©ì„ callbackí•˜ë„ë¡ ì„¤ì •
  - `EarlyStoppingCallback` ì€ on_evaluate ìˆœì„œì—ì„œ EarlyStopì„ callbackí•˜ë„ë¡ ì„¤ì •

```python
from transformers import TrainerCallback

# custom callback ë§Œë“¤ê¸°, ì´ë•Œ TrainerCallbackì„ ìƒì† ë°›ì•„ì•¼í•¨.
class myCallback(TrainerCallback):

  def on_step_begin(self, args, state, control, logs=None, **kwargs):
    # stepì€ 1íšŒ batch ì§„í–‰ì„ ì˜ë¯¸í•¨. stepì˜ ì‹œì‘ì¼ ë•Œ ì•„ë˜ì˜ ë‚´ìš©ì„ ì‹¤í–‰

      if state.global_step % args.logging_steps == 0:
        # stateëŠ” í˜„ì¬ step, epoch ë“± ì§„í–‰ ìƒíƒœì— ëŒ€í•œ ê°’ì„ ë¶ˆëŸ¬ì˜´
        # argëŠ” í›ˆë ¨ ì˜µì…˜ìœ¼ë¡œ ì„¤ì •í•œ ê°’ì„ ë¶ˆëŸ¬ì˜´.
          print("")
          print(
              f"{int(state.epoch)}ë²ˆì§¸ epoch ì§„í–‰ ì¤‘ --- {state.global_step}ë²ˆì§¸ step ê²°ê³¼"
          )
```

<br/>

#### â– Custom Trainer(ì„ íƒì‚¬í•­)

- Trainerë¥¼ í•„ìš”ì— ë§ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŒ. optimizer ì„¤ì •, loss ê³„ì‚° ë“± í›ˆë ¨ ì§„í–‰ ë°©ë²•ì— ëŒ€í•œ ë°©ë²•ì„ ìˆ˜ì •í•˜ëŠ”ë°ë„ ì‚¬ìš©í•˜ì§€ë§Œ, ëª¨ë¸ì´ ì •í™•í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ”ì§€ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì¶œë ¥ì´ í•„ìš”í•œ ê²½ìš°ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.

  ```js
  0ë²ˆì§¸ epoch ì§„í–‰ ì¤‘ ------- 0ë²ˆì§¸ step ê²°ê³¼
  input ë¬¸ì¥ : ì¥ ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [MASK] [MASK] [MASK] ì½ê³  ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤
  output ë¬¸ì¥ : ì¥ ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [##íŒŒ] [##ë²³] [##ì„]ì„ ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤

  0ë²ˆì§¸ epoch ì§„í–‰ ì¤‘ ------- 20ë²ˆì§¸ step ê²°ê³¼
  input ë¬¸ì¥ : [MASK]ì´ ì¶œê°„ëœì§€ ê½¤ ë¬ë‹¤ê³  ìƒê°í•˜ëŠ”ë° ì‹¤ìŠµí•˜ëŠ”ë° ì „í˜€ [MASK]ì—†ìŠµë‹ˆë‹¤
  output ë¬¸ì¥ : [ì±…]ì´ ì¶œê°„ëœì§€ ê½¤ ë¬ë‹¤ê³  ìƒê°í•˜ëŠ”ë° ì‹¤ìŠµí•˜ëŠ”ë° ì „í˜€ [ë¬¸ì œ]ì—†ìŠµë‹ˆë‹¤

  ```

- Train ë‹¨ê³„ì—ì„œ ëª¨ë¸ì— input dataë¥¼ ë„£ê³  output dataë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì€ compute_loss ë§¤ì„œë“œì—ì„œ ì´ë¤„ì§. ë”°ë¼ì„œ compute_loss ë§¤ì„œë“œë¥¼ ë®ì–´ì“°ê¸°í•˜ì—¬ í•„ìš”í•œ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŒ.

```python
class customtrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    # compute_loss í•¨ìˆ˜ ë®ì–´ì“°ê¸°
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        How the loss is computed by Trainer.

        By default, all models return the loss in the first element.

        Subclass and override for custom behavior.
        """
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None
        prds,outputs = model(inputs)
        # Save past state if it exists
        # TODO: this needs to be fixed and made cleaner later.
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            # We don't use .loss here since the model may return tuples instead of ModelOutput.
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]

        # ############# ëª¨ë¸ í•™ìŠµ ê³¼ì • í™•ì¸ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€

        if self.state.global_step % self.args.logging_steps == 0:
            # self.state.global_step = í˜„ step íŒŒì•…
            # args.logging_steps = argumentì—ì„œ ì§€ì •í•œ logging_step

            # batch ì¤‘ 0 ë²ˆì§¸ ìœ„ì¹˜í•œ ë¬¸ì¥ ì„ íƒ
            num = 1
            input_id = inputs.input_ids[num].reshape(-1).data.tolist()
            output_id = outputs.logits[num].argmax(dim=-1).reshape(-1).data.tolist()
            attention_mask = inputs.attention_mask[num]

            # maskê°€ ìœ„ì¹˜í•œ idx ì¶”ì¶œí•˜ê¸°
            mask_idx = (inputs.input_ids[num] == 4).nonzero().data.reshape(-1).tolist()

            # padding ì œê±°
            input_id_without_pad = [
                input_id[i] for i in range(len(input_id)) if attention_mask[i]
            ]
            output_id_without_pad = [
                output_id[i] for i in range(len(output_id)) if attention_mask[i]
            ]

            # id to token
            # [1:-1] [CLS,SEP] ì œê±°
            inputs_tokens = self.tokenizer.convert_ids_to_tokens(input_id_without_pad)[
                1:-1
            ]
            outputs_tokens = self.tokenizer.convert_ids_to_tokens(
                output_id_without_pad
            )[1:-1]

            # output mask ë¶€ë¶„ í‘œì‹œí•˜ê¸°
            for i in mask_idx:
                # [CLS,SEP ìœ„ì¹˜ ì¡°ì •]
                outputs_tokens[i - 1] = "[" + outputs_tokens[i - 1] + "]"

            inputs_sen = self.tokenizer.convert_tokens_to_string(inputs_tokens)
            outputs_sen = self.tokenizer.convert_tokens_to_string(outputs_tokens)

            print(f"input ë¬¸ì¥ : {''.join(inputs_sen)}")
            print(f"output ë¬¸ì¥ : {''.join(outputs_sen)}")

            # input ë¬¸ì¥ : ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [MASK] [MASK] [MASK] ì½ê³  ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤
            # output ë¬¸ì¥ : ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [##íŒŒ] [##ë²³] [##ì„]ì„ ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤

        return (loss, outputs) if return_outputs else loss
```

<br/>

#### â– Trainer ì„¤ì •

- ì•ì„œ ì„¤ì •í–ˆë˜ ì˜µì…˜, ë°ì´í„°ì…‹, callback í•¨ìˆ˜ ë“±ì„ trainerë¡œ í†µí•©í•˜ëŠ” ê³¼ì •ì„.
- customtrainerë¥¼ Trainerë¡œ ì‚¬ìš©í–ˆê³ , ëª¨ë¸ í•™ìŠµ ê³¼ì • í™•ì¸ ë‹¨ê³„ì—ì„œ tokenizerê°€ í•„ìš”í•˜ë¯€ë¡œ tokenizerë¥¼ í¬í•¨í–ˆìŒ.
- callback í•¨ìˆ˜ëŠ” 1ê°œë¥¼ ë¶ˆëŸ¬ì˜¤ë”ë¼ë„ list íƒ€ì…ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì•¼í•¨.
- trainerë¥¼ ì •ì˜í•œ ë’¤ .train() ë§¤ì„œë“œë¥¼ ì‹¤í–‰í•˜ë©´ í•™ìŠµ ì‹œì‘

```python
trainer = customtrainer(
    model=model.to(device),
    train_dataset=train_data_set,
    eval_dataset=validation_data_set,
    data_collator=data_collator_BERT,
    args=training_args,
    tokenizer=tokenizer,
    callbacks=[myCallback,PrinterCallback],
)

trainer.train()
```

- TrainerëŠ” í•™ìŠµ ê³¼ì •ê³¼ Training lossë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆë„ë¡ interfaceë¥¼ ì§€ì›í•¨.

<img src='img/interface.png'/>
