---
title: "ELECTRA í•™ìŠµ êµ¬ì¡° ì†Œê°œ ë° Domain Adaptation ìˆ˜í–‰í•˜ê¸°"
category: "DeepLearning"
date: "2022-12-22"
thumbnail: "./img/electra.png"
desc: "pytorchë¥¼ í™œìš©í•´ ELECTRA ë…¼ë¬¸ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©° Generatorì™€ Descriminator ê°„ ì—°ê²° ë°©ë²• ë° Replace Token Detection(RTD)ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤. Huggingfaceì˜ trainerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•˜ê³ , ì´ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì„ ì œì‘í•´ ELECTRA ë¿ë§Œ ì•„ë‹ˆë¼ Huggingface ì‚¬ìš©ë²•ì„ ì†ì‰½ê²Œ ìµí ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤. ì§ì ‘ Domain Adapatationì„ ê²½í—˜í•˜ë©° ELECTRA í•™ìŠµ ë°©ë²• ë° ë°ì´í„° íë¦„ì— ëŒ€í•´ ì´í•´í•  ìˆ˜ ìˆë‹¤."
---

### ë“¤ì–´ê°€ë©°

ì´ ê¸€ì€ ELECTRAë¥¼ ğŸ¤— Transformersë¥¼ í™œìš©í•´ Domain Adaptationí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ì™¸ì—ë„ ğŸ¤— Transformers ê°œë…ê³¼ Trainer, Dataset ë“± ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²• ë˜í•œ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ ğŸ¤— Transformersì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²½ìš°ì—ë„ ì´ ê¸€ì„ ì°¸ê³ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Domain Adaptation ê°œë…ì„ ì•Œê³ ì‹¶ë‹¤ë©´ [[NLP] Further Pre-training ë° Fine-tuning ì •ë¦¬](https://yangoos57.github.io/blog/DeepLearning/paper/Finetuning/Finetuning)ë¥¼ ì°¸ê³ ë°”ëë‹ˆë‹¤.

- ELECTRA í•™ìŠµ êµ¬ì¡°ëŠ” [lucidrainsì˜ electra-pytorch](https://github.com/lucidrains/electra-pytorch) ì½”ë“œë¥¼ ì°¸ê³ í–ˆìœ¼ë©°, ğŸ¤— Transformersë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì¼ë¶€ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

- ELECTRA Base ëª¨ë¸ì€ monologgë‹˜ì˜ `koelectra-v-base`ëª¨ë¸ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

- ELECTRA ëª¨ë¸ í•™ìŠµì— ëŒ€í•œ Jupyter Notebookì€ [ë§í¬](https://github.com/yangoos57/Electra_for_Domain_Adaptation/blob/main/%5Btutorial%5D%20domain%20adaptation.ipynb)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.

### ì™œ ELECTRAì¸ê°€?

ELECTRAëŠ” Masked Language Model(MLM)ì˜ ë¹„íš¨ìœ¨ì ì¸ í•™ìŠµ ë°©ë²•ì— ìƒˆë¡œìš´ ëŒ€ì•ˆì„ ì œì‹œí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ELECTRAì˜ í•™ìŠµ ë°©ë²•ì¸ replaced token detection(RTD)ì€ ë™ì¼ í™˜ê²½, ë™ì¼ ì‹œê°„ ëŒ€ë¹„ MLM ë³´ë‹¤ ë” ì¢‹ì€ í•™ìŠµ ì„±ëŠ¥ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì´ëŠ” ë™ì¼í•œ ì„±ëŠ¥ì„ ë‚´ê¸°ìœ„í•´ RTDê°€ Masked Language Model(MLM)ì— ë¹„í•´ ë” ì ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ í•„ìš”ë¡œ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ELECTRAëŠ” í•™ìŠµì˜ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ ìµœì í™”ë¥¼ ì´ë¤„ë‚¸ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ELECTRAë¥¼ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì—ì„œëŠ” MLM ëª¨ë¸ì´ ë¹„íš¨ìœ¨ì ì¸ ì´ìœ ì— ëŒ€í•´ ë¬¸ì¥ì˜ 15%ë§Œì´ í•™ìŠµì— í™œìš©í•˜ê¸° ë•Œë¬¸ì´ë¼ ì§€ì í•©ë‹ˆë‹¤. MLM ëª¨ë¸ì€ [Mask]ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ë°, ë¬¸ì¥ì˜ ì•½ 15% í† í°ì´ ì„ì˜ë¡œ ì„ íƒ ëœ ë’¤ ì „í™˜ë˜ë¯€ë¡œ [Mask] ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ 85% ë¬¸ì¥ì€ í•™ìŠµì„ í•˜ì§€ ì•Šê²Œ ë˜ì–´ ë¹„íš¨ìœ¨ì´ ë°œìƒí•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¹„íš¨ìœ¨ì„ ê°œì„ í•˜ê³ ì ë“±ì¥í•œ ë°©ë²•ì„ ì ìš©í•œ ëª¨ë¸ì´ ELECTRAì´ë©°, ë¹„íš¨ìœ¨ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ replaced token detection(RTD)ë¼ ë¶€ë¦…ë‹ˆë‹¤. RTDëŠ” ì•„ë˜ì™€ ì ˆì°¨ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

- Generator ëª¨ë¸ì—ì„œ ë¬¸ì¥ í† í° ì¤‘ ì•½ 15%ë¥¼ ë°”ê¿” ê°€ì§œ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤. ì´ë•Œ GeneratorëŠ” ê¸°ì¡´ MLM í•™ìŠµ ë°©ë²•ëŒ€ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. Generatorê°€ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ì´ìœ ëŠ” ë” ë‚˜ì€ ê°€ì§œ ë¬¸ì¥ì„ ë§Œë“¤ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

- DiscriminatorëŠ” ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ ì§„ì§œ í† í°ì¸ì§€, ê°€ì§œ í† í°ì¸ì§€ êµ¬ë³„í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•™ìŠµ ë°©ë²•ì€ ëª¨ë“  ë¬¸ì¥ì„ ê²€ì¦í•´ì•¼í•˜ë¯€ë¡œ MLMê³¼ ë¹„êµí–ˆì„ ë•Œ ë™ì¼í•œ ë¬¸ì¥ ëŒ€ë¹„ ë” ë§ì€ í•™ìŠµì´ ì´ë¤„ì§€ê²Œ ë©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ê°™ì€ ì–‘ì˜ ë°ì´í„°, í¬ê¸°ë¼ í• ì§€ë¼ë„ ë” ë¹ ë¥¸ ì„±ëŠ¥ í–¥ìƒì´ ê°€ëŠ¥í•œ ê²ƒì…ë‹ˆë‹¤.

- í•™ìŠµì„ ì™„ë£Œí–ˆë‹¤ë©´ GeneratorëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³  Discriminatorë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ í™œìš©í•©ë‹ˆë‹¤.

### Domain Adaptation ì´í•´í•˜ê¸°

Domain Adaptationì€ Pre-trained ëª¨ë¸ì„ íŠ¹ì • ë¶„ì•¼(Domain)ì— ì í•©í•œ ëª¨ë¸ë¡œ ê°œì„ í•˜ê¸° ìœ„í•œ ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Domain Adaptationì€ ì¢…ì¢… Further Pre-trainingì´ë¼ëŠ” ìš©ì–´ë¡œë„ ì‚¬ìš©ë˜ê³¤ í•˜ëŠ”ë°, ì´ëŠ” Domain Adaptationì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ Pre-trained Modelì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼í•˜ë¯€ë¡œ Pre-trainingì„ ì§€ì†í•œë‹¤ëŠ” ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.

Domain Adaptationì˜ ìœ ì˜ì–´ê°€ Further Pre-trainingì´ë¼ëŠ” ì ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯, Domain Adaptationì„ êµ¬í˜„í•¨ìœ¼ë¡œì„œ Pre-trainingì´ ì§„í–‰ë˜ëŠ” ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ê¸€ì€ ELECTRAì— ëŒ€í•´ Domain Adaptationì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ì§€ë§Œ ì‚¬ìš©ìì— í•„ìš”ì— ë”°ë¼ì„  ì´ ë°©ë²•ì„ Pre-trainingì„ ìœ„í•´ ì ìš©í•´ ìƒˆë¡œìš´ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

> Domain Adaptationì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° [[NLP] Domain Adaptationê³¼ Finetuning ê°œë… ì •ë¦¬](https://yangoos57.github.io/blog/DeepLearning/paper/Finetuning/Finetuning/)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

<br/>

## ELECTRA í•™ìŠµ êµ¬ì¡° ì œì‘í•˜ê¸°

ì•ì„œ ELECTRA ëª¨ë¸ì€ RTD ë°©ë²•ì„ ì ìš©í•œ ëª¨ë¸ì„ ì˜ë¯¸í•˜ë©° RTDëŠ” Generatorê°€ ë§Œë“  ê°€ì§œ ë¬¸ì¥ì„ Discriminatorê°€ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë¼ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ì´ì œëŠ” RTD êµ¬í˜„ì— í•„ìš”í•œ Generatorì™€ Discriminatorë¥¼ ğŸ¤— Transformersë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ê³¼ RTDë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

### ğŸ¤— Transformersë¡œ Generator, Discriminator ë¶ˆëŸ¬ì˜¤ê¸°

ğŸ¤— Transformersë¡œ Generator, Discriminatorë¥¼ ë§Œë“¤ê¸° ì „ì— ğŸ¤— Transformersì´ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ğŸ¤— Transformersì˜ ì¥ì ì€ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” Taskì— ì í•©í•œ êµ¬ì¡°ë¥¼ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì— ìˆìŠµë‹ˆë‹¤. ğŸ¤— Transformersì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” êµ¬ì¡°ëŠ” Bertì˜ ê²½ìš° `MaskedLM`, `SequenceClassification`, `MultipleChoice`, `TokenClassification`, `QuestionAnswering` ì´ ìˆìŠµë‹ˆë‹¤.(ì–¸ì–´ ëª¨ë¸ë³„ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” êµ¬ì¡°ëŠ” ìƒì´í•©ë‹ˆë‹¤.) ì´ëŸ¬í•œ êµ¬ì¡°ë“¤ì€ `BaseModel`ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜ ì¶œë ¥ ìƒë‹¨(output-Layer) êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ê¸°ì¡´ì— ë§Œë“¤ì–´ì§„ Layerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ Layerë¥¼ êµ¬ì„±í•´ì•¼í•œë‹¤ë©´ í•„ìš”í•œ Layerë¥¼ ìƒì„±í•œ ë’¤ BaseModelì„ ë¶ˆëŸ¬ì™€ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img src='img/img2.png' alt='img2'>

<br/>
<br/>
<br/>

ì´ë ‡ê²Œ ì¶œë ¥ ìƒë‹¨ êµ¬ì¡°ê°€ ë‹¤ì–‘í•œ ì´ìœ ëŠ” Task ë³„ë¡œ í•„ìš”í•œ Output í˜•íƒœê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì˜ˆë¡œë“¤ì–´ MaskedLM êµ¬ì¡°ì˜ ê²½ìš° input dataì— ì¡´ì¬í•˜ëŠ” [MASK]ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë“¤ì˜ ìˆœìœ„ë¥¼ Outputìœ¼ë¡œ ì¶œë ¥í•´ì•¼í•©ë‹ˆë‹¤. ë°˜ë©´ Sequence Classificationì€ ë¬¸ì¥ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê±°ë‚˜ í™•ë¥ ì„ ì˜ˆì¸¡í•´ì•¼í•˜ëŠ” êµ¬ì¡°ì—ì„œ í™œìš©í•´ì•¼ í•˜ë¯€ë¡œ 0~1 ë²”ìœ„ì˜ ê°’(Regression ëª¨ë¸), ë˜ëŠ” ì •ìˆ˜ê°’(Classification ëª¨ë¸)ì˜ Outuputì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ğŸ¤— Transformerì˜ ê¸°ë³¸ êµ¬ì¡°ì— ëŒ€í•´ ì–´ëŠì •ë„ íŒŒì•…í–ˆìœ¼ë‹ˆ ìš°ë¦¬ê°€ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ëª¨ë¸ì´ ì–´ë– í•œ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•˜ëŠ”ì§€ë¡œ ì£¼ì œë¥¼ ì¢í˜€ë³´ê² ìŠµë‹ˆë‹¤. ELECTRA í•™ìŠµì— í•„ìš”í•œ Generatorì™€ Discriminatorë¥¼ ğŸ¤— Transformerì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ì„  `ElectraForMaskedLM`ì™€ `ElectraForPreTraining`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

`ElectraForMaskedLM`ëŠ” Generatorê°€ ê°€ì§œ ë¬¸ì¥ì„ ìƒì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í† í°ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ë©° `ElectraForPreTraining`ëŠ” Discriminatorê°€ í•™ìŠµì— í•„ìš”í•œ tokenì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë³„í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

```python

from transformers import ElectraForPreTraining, ElectraTokenizer, ElectraForMaskedLM

tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-generator")

generator = ElectraForMaskedLM.from_pretrained('monologg/koelectra-base-v3-generator')

discriminator = ElectraForPreTraining.from_pretrained("monologg/koelectra-base-v3-discriminator")

```

<br/>
<br/>

#### â– ElectraForMaskedLM êµ¬ì¡° ì‚´í´ë³´ê¸°(Generator)

ì•„ë˜ ì½”ë“œëŠ” ğŸ¤— Transformersì˜ ElectraForMaskedLM Classë¥¼ ë³µì‚¬í•œ ê²ƒì…ë‹ˆë‹¤. `__init__` ë§¤ì„œë“œë¥¼ ElectraForMaskedLMëŠ” ë³´ë©´ electra ëª¨ë¸ì„ ğŸ¤— Transformersì˜ ElectraModel ëª¨ë“ˆì—ì„œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ElectraModelì˜ Outputì€ Encoder ë§ˆì§€ë§‰ë‹¨ì˜ outputì„ ì˜ë¯¸í•˜ëŠ” 'last-hidden-state'ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ElectraModelì˜ 'last-hidden-state'ëŠ” generator_predictions layerì™€ generator_lm_head layerë¥¼ ê±°ì³ [MASK] í† í°ì— ì•Œë§ì€ í† í°ì„ í™•ë¥ ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

```python

# Huggingface Transformers ë‚´ë¶€ ElectraForMaskedLM ì½”ë“œ

class ElectraForMaskedLM(ElectraPreTrainedModel):
    _keys_to_ignore_on_load_missing = ["generator_lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)

        # ElectraForMaskedLM ë‚´ë¶€ì—ì„œ ElectraModel ëª¨ë“ˆì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•¨.
        self.electra = ElectraModel(config)

        # [Mask] í† í°ì— ì í•©í•œ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” generator predictorê°€ í¬í•¨ë˜ì–´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ.
        self.generator_predictions = ElectraGeneratorPredictions(config)

        self.generator_lm_head = nn.Linear(config.embedding_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(...) :

            # electraë¥¼ í†µí•´ ë§ˆì§€ë§‰ encoderì˜ output(=last_hidden_state)ë¥¼ ë°›ìŒ.
            generator_hidden_states = self.electra(
                input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                position_ids=position_ids,
                head_mask=head_mask,
                inputs_embeds=inputs_embeds,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )
            # last_hidden_statesë¥¼ prediction scoresì— input ë°ì´í„°ë¡œ í™œìš©í•¨.
            generator_sequence_output = generator_hidden_states[0]

            # last_hidden_statesë¥¼ MLM prediction layerì˜ input ë°ì´í„°ë¡œ í™œìš©í•¨.
            prediction_scores = self.generator_predictions(generator_sequence_output)
            prediction_scores = self.generator_lm_head(prediction_scores)

    return MaskedLMOutput(
                loss=loss,
                # Prediction_scoreì„ ë¦¬í„´
                # logitsì˜ shapeì€ (batch_size, src_token_len, vocab_size)
                logits=prediction_scores,
                hidden_states=generator_hidden_states.hidden_states,
                attentions=generator_hidden_states.attentions,
            )

```

<br/>
<br/>

#### â– ElectraForPreTraining êµ¬ì¡° ì‚´í´ë³´ê¸°(Discriminator)

Discriminatorê°€ ìˆ˜í–‰í•˜ëŠ” í† í°ì˜ ì§„ìœ„ì—¬ë¶€ íŒë³„ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ëŠ¥ì€ `ElectraForPreTraining`ì´ ë‹´ë‹¹í•©ë‹ˆë‹¤. ElectraForPreTrainingì˜ `__init__` ë§¤ì„œë“œë„ ë§ˆì°¬ê°€ì§€ë¡œ `ElectraModel` ëª¨ë“ˆì„ ë² ì´ìŠ¤ë¡œ í•˜ê³ ìˆìœ¼ë©°, ë¬¸ì¥ ë‚´ ê°œë³„ tokenì˜ ì§„ìœ„ ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” classification layerê°€ ì—°ê²°ë˜ì–´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë•Œ Classification LayerëŠ” ê°œë³„ í† í°ì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ 0ê³¼ 1ë¡œ íŒë‹¨í•˜ë©° outputì´ 1ì¸ ê²½ìš° ëª¨ë¸ì´ ê°€ì§œ í† í°ìœ¼ë¡œ íŒë³„í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

```python

# Huggingface Transformers ë‚´ë¶€ ElectraForPreTraining ì½”ë“œ

class ElectraForPreTraining(ElectraPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        # ElectraForPreTraining ë‚´ë¶€ì—ì„œ ElectraModel ëª¨ë“ˆì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•¨.
        self.electra = ElectraModel(config)

        # Tokenì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” Precdiction ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´.
        self.discriminator_predictions = ElectraDiscriminatorPredictions(config)

    ....


    def forward(...)
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # electraë¥¼ í†µí•´ ë§ˆì§€ë§‰ encoderì˜ output(=last_hidden_state)ë¥¼ ë°›ìŒ.
        discriminator_hidden_states = self.electra(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # last_hidden_statesë¥¼ classification layerì˜ input ë°ì´í„°ë¡œ í™œìš©í•¨.
        discriminator_sequence_output = discriminator_hidden_states[0]

        logits = self.discriminator_predictions(discriminator_sequence_output)

        return ElectraForPreTrainingOutput(
            loss=loss,
            # logitsì„ ë¦¬í„´
            # logitsì˜ shapeì€ (batch_size, src_token_len)
            logits=logits,
            hidden_states=discriminator_hidden_states.hidden_states,
            attentions=discriminator_hidden_states.attentions,
        )
```

<br>

#### â– ëª¨ë¸ì„ í•™ìŠµí•œ ë‹¤ìŒì€?

ELECTRA í•™ìŠµ êµ¬ì¡°ì— ëŒ€í•œ ì„¤ëª…ì„ ì´ì–´ë‚˜ê°€ê¸° ì „ì—, í•™ìŠµì„ ì™„ë£Œí•œ ë‹¤ìŒ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìš°ì„ ì ìœ¼ë¡œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì‹¤ì œ í•™ìŠµì„ ìˆ˜í–‰í•˜ê¸°ë„ ì „ì— í•™ìŠµ ì´í›„ë¥¼ ì†Œê°œí•˜ëŠ” ì´ìœ ëŠ” ì•ì„œ ì„¤ëª…í•œ ğŸ¤— Transformerì˜ êµ¬ì¡°ë¥¼ ë³µìŠµ ì°¨ì›ì—ì„œ ë‹¤ì‹œ í•œë²ˆ ì„¤ëª…í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

ì§€ê¸ˆê¹Œì§€ ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ í•µì‹¬ì¸ Discriminatorì™€ Generatorë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì— ëŒ€í•´ ë°°ì› ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ Electra Modelì— layerê°€ ì˜¬ë¼ê°„ êµ¬ì¡°ì„ì„ ë‚´ë¶€ ì½”ë“œë¥¼ í†µí•´ ì´í•´í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆë‹¤ë©´, Electa Modelì— Layerë¥¼ ì—°ê²°í–ˆë˜ ê²ƒê³¼ ë°˜ëŒ€ë¡œ, ì—°ê²°ëœ Layerë¥¼ ì œê±°í•´ Electra Modelë§Œì„ í™œìš©í•´ì•¼í•©ë‹ˆë‹¤. ì´ë•Œ Discriminatorë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ `ElectraForPreTraining` ë‚´ë¶€ì— ìˆëŠ” ElectraModelë§Œì„ ì¶”ì¶œí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ì¶”ì¶œí•œ ëª¨ë¸ì€ ìˆ˜í–‰í•´ì•¼í•˜ëŠ” Taskì— ì í•©í•œ Layerì— ì—°ê²°í•´ Fine-tuningí•˜ì—¬ í™œìš©í•˜ê²Œ ë©ë‹ˆë‹¤.

```python
discriminator = ElectraForPreTraining.from_pretrained('...')

# Electra Model ì¶”ì¶œí•˜ê¸°
trained_electra = discriminator.electra
```

<br/>
<br/>

### ELECTRA í•™ìŠµ êµ¬ì¡° ì„¤ê³„í•˜ê¸°

ë³¸ê²©ì ìœ¼ë¡œ ELECTRA í•™ìŠµ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³  ëª¨ë¸ì´ ì–´ë– í•œ ë°©ë²•ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ ELECTRA ë…¼ë¬¸ì— ìˆëŠ” RTDì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ì•„ë˜ ë„ì‹í™”ëŠ” ë§¤ìš° ê°„ë‹¨í•´ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì€ ìƒê°ë³´ë‹¤ ê°„ë‹¨í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ELECTRA í•™ìŠµ êµ¬ì¡°ëŠ” í¬ê²Œ 3ë‹¨ê³„ë¡œ êµ¬ë¶„ë˜ë©° 1ë‹¨ê³„ëŠ” input data masking, 2ë‹¨ê³„ëŠ” Generator í•™ìŠµ ë° fake sentence ìƒì„±, 3ë‹¨ê³„ëŠ” Discriminator í•™ìŠµì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ê³„ë³„ ì„¤ëª…ì€ ì½”ë“œ ì£¼ì„ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

<img src='img/electra_sm.png'/>

<br/>

```python
import math
from functools import reduce
from collections import namedtuple

import torch
from torch import nn
import torch.nn.functional as F

# constants

Results = namedtuple(
    "Results",
    [
        "loss",
        "mlm_loss",
        "disc_loss",
        "gen_acc",
        "disc_acc",
        "disc_labels",
        "disc_predictions",
    ],
)

# ëª¨ë¸ ë‚´ë¶€ì—ì„œ í™œìš©ë˜ëŠ” í•¨ìˆ˜ ì •ì˜

def log(t, eps=1e-9):
    return torch.log(t + eps)

def gumbel_noise(t):
    noise = torch.zeros_like(t).uniform_(0, 1)
    return -log(-log(noise))

def gumbel_sample(t, temperature=1.0):
    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)

def prob_mask_like(t, prob):
    return torch.zeros_like(t).float().uniform_(0, 1) < prob

def mask_with_tokens(t, token_ids):
    init_no_mask = torch.full_like(t, False, dtype=torch.bool)
    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)
    return mask

def get_mask_subset_with_prob(mask, prob):
    batch, seq_len, device = *mask.shape, mask.device
    max_masked = math.ceil(prob * seq_len)

    num_tokens = mask.sum(dim=-1, keepdim=True)
    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()
    mask_excess = mask_excess[:, :max_masked]

    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)
    _, sampled_indices = rand.topk(max_masked, dim=-1)
    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)

    new_mask = torch.zeros((batch, seq_len + 1), device=device)
    new_mask.scatter_(-1, sampled_indices, 1)
    return new_mask[:, 1:].bool()

# main electra class

class Electra(nn.Module):
    def __init__(
        self,
        generator,
        discriminator,
        tokenizer,
        *,
        num_tokens=35000,
        mask_prob=0.15,
        replace_prob=0.85,
        mask_token_id=4,
        pad_token_id=0,
        mask_ignore_token_ids=[2, 3],
        disc_weight=50.0,
        gen_weight=1.0,
        temperature=1.0,
    ):
        super().__init__()

        """
        num_tokens: ëª¨ë¸ vocab_size
        mask_prob: í† í° ì¤‘ [MASK] í† í°ìœ¼ë¡œ ëŒ€ì²´ë˜ëŠ” ë¹„ìœ¨
        replace_prop:  í† í° ì¤‘ [MASK] í† í°ìœ¼ë¡œ ëŒ€ì²´ë˜ëŠ” ë¹„ìœ¨
        mask_token_i: [MASK] Token id
        pad_token_i: [PAD] Token id
        mask_ignore_token_id: [CLS],[SEP] Token id
        disc_weigh: discriminator lossì˜ Weight ì¡°ì •ì„ ìœ„í•œ ê°’
        gen_weigh: generator lossì˜ Weight ì¡°ì •ì„ ìœ„í•œ ê°’
        temperature: gumbel_distributionì— í™œìš©ë˜ëŠ” arg, ê°’ì´ ë†’ì„ìˆ˜ë¡ ëª¨ì§‘ë‹¨ ë¶„í¬ì™€ ìœ ì‚¬í•œ sampling ìˆ˜í–‰
        """

        # Generator, Discriminator, Tokenizer
        self.generator = generator
        self.discriminator = discriminator
        self.tokenizer = tokenizer

        # mlm related probabilities
        self.mask_prob = mask_prob # 0.15
        self.replace_prob = replace_prob # 0.85

        self.num_tokens = num_tokens # 35000

        # token ids
        self.pad_token_id = pad_token_id
        self.mask_token_id = mask_token_id
        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])

        # sampling temperature
        self.temperature = temperature

        # loss weights
        # Discriminor Weightì´ 50ì¸ ì´ìœ ëŠ” ì˜¤ì°¨ì— ëŒ€í•œ Lossê°€ Generator ë³´ë‹¤ ì‘ê¸° ë•Œë¬¸
        # ë”°ë¼ì„œ ì ì ˆí•œ í•™ìŠµì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Weightì„ í†µí•´ Discriminatorì™€ Generatorì˜ ì°¨ì´ë¥¼ ì¤„ì—¬ì¤Œ
        self.disc_weight = disc_weight # 50.0
        self.gen_weight = gen_weight # 1.0

    def forward(self, input_ids, **kwargs):

        input = input_ids["input_ids"]

        # ------ 1ë‹¨ê³„ Input Data Masking --------#

        """
        - GeneratorëŠ” Bertì™€ êµ¬ì¡°ë„ ë™ì¼í•˜ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ë„ ë™ì¼í•¨.

        - Generator í•™ìŠµì„ ìœ„í•´ì„  [Masked] í† í°ì´ í•„ìš”í•˜ë¯€ë¡œ input dataë¥¼ Maskingí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•¨.

        """

        replace_prob = prob_mask_like(input, self.replace_prob)

        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])
        # also do not include these special tokens in the tokens chosen at random
        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)
        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)

        # get mask indices
        mask_indices = torch.nonzero(mask, as_tuple=True)

        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)
        masked_input = input.clone().detach()

        # set inverse of mask to padding tokens for labels
        gen_labels = input.masked_fill(~mask, self.pad_token_id)

        # clone the mask, for potential modification if random tokens are involved
        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens
        masking_mask = mask.clone()

        # [mask] input
        masked_input = masked_input.masked_fill(
            masking_mask * replace_prob, self.mask_token_id
        )

        # ------ 2ë‹¨ê³„ Masking ëœ ë¬¸ì¥ì„ Generatorê°€ í•™ìŠµí•˜ê³  ê°€ì§œ Tokenì„ ìƒì„± --------#

        """
        - Generatorë¥¼ í•™ìŠµí•˜ì—¬ MLM_loss ê³„ì‚°(combined_loss ê³„ì‚°ì— í™œìš©)
        - Generatorì—ì„œ ì˜ˆì¸¡í•œ ë¬¸ì¥ì„ Discriminator í•™ìŠµì— í™œìš©
        - ex) ì›ë³¸ ë¬¸ì¥ : íŠ¹íˆ ì•ˆë“œë¡œì´ë“œ í”Œë«í¼ ê¸°ë°˜ì˜ (ì›¹)ì•±ê³¼ (í•˜ì´)ë¸Œë“œë¦¬ì•±ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤
              ê°€ì§œ ë¬¸ì¥ : íŠ¹íˆ ì•ˆë“œë¡œì´ë“œ í”Œë«í¼ ê¸°ë°˜ì˜ (ë§ˆì´í¬ë¡œ)ì•±ê³¼ (ì´)ë¸Œë“œë¦¬ì•±ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤
        """

        # get generator output and get mlm loss(ìˆ˜ì •)
        logits = self.generator(masked_input, **kwargs).logits

        mlm_loss = F.cross_entropy(
            logits.transpose(1, 2), gen_labels, ignore_index=self.pad_token_id
        )

        # use mask from before to select logits that need sampling
        sample_logits = logits[mask_indices]

        # sample
        sampled = gumbel_sample(sample_logits, temperature=self.temperature)

        # scatter the sampled values back to the input
        disc_input = input.clone()
        disc_input[mask_indices] = sampled.detach()

        # generate discriminator labels, with replaced as True and original as False
        disc_labels = (input != disc_input).float().detach()

        # ------ 3ë‹¨ê³„ Tokenì˜ ì§„ìœ„ì—¬ë¶€ë¥¼ Discriminatorê°€ ì˜ˆì¸¡í•˜ê³  ì´ë¥¼ í•™ìŠµ --------#

        """
        - ê°€ì§œ ë¬¸ì¥ì„ í•™ìŠµí•´ ê°œë³„ í† í°ì— ëŒ€í•´ ì§„ìœ„ì—¬ë¶€ë¥¼ íŒë‹¨
        - ì§„ì§œ tokenì´ë¼ íŒë‹¨í•˜ë©´ 0, ê°€ì§œ í† í°ì´ë¼ íŒë‹¨í•˜ë©´ 1ì„ ë¶€ì—¬
        - ì •ë‹µê³¼ ë¹„êµí•´ disc_lossë¥¼ ê³„ì‚°(combined_loss ê³„ì‚°ì— í™œìš©)
        - combined_loss : í•™ìŠµì˜ ìµœì¢… lossì„. ëª¨ë¸ì€ combined_lossì˜ ìµœì†Ÿê°’ì„ ì–»ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ì§„í–‰
        """

        # get discriminator predictions of replaced / original
        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)

        # get discriminator output and binary cross entropy loss
        disc_logits = self.discriminator(disc_input, **kwargs).logits
        disc_logits_reshape = disc_logits.reshape_as(disc_labels)

        disc_loss = F.binary_cross_entropy_with_logits(
            disc_logits_reshape[non_padded_indices], disc_labels[non_padded_indices]
        )

        combined_loss = (self.gen_weight * mlm_loss + self.disc_weight * disc_loss,)

        # ------ ëª¨ë¸ ì„±ëŠ¥ ë° í•™ìŠµ ê³¼ì •ì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§€í‘œ(Metrics) ì„¤ê³„(ì„ íƒì‚¬í•­) --------#

        with torch.no_grad():
            # gen mask ì˜ˆì¸¡
            gen_predictions = torch.argmax(logits, dim=-1)

            # fake token ì§„ìœ„ ì˜ˆì¸¡
            disc_predictions = torch.round(
                (torch.sign(disc_logits_reshape) + 1.0) * 0.5
            )
            # generator_accuracy
            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()
            # discriminator_accuracy
            disc_acc = (
                0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean()
                + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()
            )


        return Results(
            combined_loss,
            mlm_loss,
            disc_loss,
            gen_acc,
            disc_acc,
            disc_labels,
            disc_predictions,
        )
```

<br/>

### ğŸ¤— Datasetsìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

ELECTRA í•™ìŠµ êµ¬ì¡°ë¥¼ ì„¤ê³„í–ˆìœ¼ë‹ˆ ì´ì œ Domain Adaptationì— í•„ìš”í•œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ğŸ¤— Transformersì˜ Trainer APIë¥¼ í™œìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•  ì˜ˆì •ì´ë¯€ë¡œ ğŸ¤— Datasetsì„ í™œìš©í•©ë‹ˆë‹¤. Trainerì— pytorchì˜ Datasetì„ ì‚¬ìš©í•˜ëŠ”ë° ë¬¸ì œëŠ” ì—†ì§€ë§Œ, ê°œì¸ì ì¸ ê²½í—˜ì„ ë¹„ì¶”ì–´ ë´¤ì„ ë•Œ ì›ì¸ì„ ì°¾ê¸° ì–´ë ¤ìš´ ì—ëŸ¬ë¡œ ì¸í•´ ë””ë²„ê¹…í•˜ê¸° ì–´ë ¤ì›Œ ì¶”ì²œí•˜ëŠ” ì¡°í•©ì€ ì•„ë‹™ë‹ˆë‹¤.

í•™ìŠµì— í™œìš©í•˜ëŠ” ë°ì´í„°ëŠ” [ë§í¬](https://github.com/yangoos57/Electra_for_Domain_Adaptation)ì—ì„œ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from datasets import load_dataset

# local fileì„ ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ì„ë• 'í™•ì¥ìëª…', 'ê²½ë¡œ' ì ê¸°
train = load_dataset('csv',data_files='data/book_train_128.csv')
validation = load_dataset('csv',data_files='data/book_validation_128.csv')

# tokenizing ë°©ë²• ì •ì˜
def tokenize_function(examples):
    return tokenizer(examples['sen'], max_length=128, padding=True, truncation=True)

# datasetsì˜ map ë§¤ì„œë“œë¥¼ í™œìš©í•´ ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
train_data_set = train['train'].map(tokenize_function,batch_size=True)
validation_data_set = validation['train'].map(tokenize_function,batch_size=True)
```

<br/>
<br/>
<br/>

**datasets ê¸°ë³¸ ë§¤ì„œë“œ ì†Œê°œ**

```python
train = load_dataset('csv',data_files='data/book_train_128.csv')

print(train)

>>> DatasetDict({
    train: Dataset({
        features: ['Unnamed: 0', 'sen'],
        num_rows: 175900
    })
})

#----------

# column ì œê±°
train = train.remove_columns('Unnamed: 0')

print(train)

>>> DatasetDict({
    train: Dataset({
        features: ['sen'],
        num_rows: 175900
    })
})

#----------

# train ë°ì´í„°ì…‹ìœ¼ë¡œ ì´ë™
train_data_set = train['train']

print(train_data_set)

>>> Dataset({
    features: ['Unnamed: 0', 'sen'],
    num_rows: 175900
})

#----------

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
print(train_data_set[0])

>> {'sen': 'ì´ ì±…ì˜ íŠ¹ì§•ã†ì½”ë”©ì˜ ê¸°ì´ˆ ê¸°ì´ˆìˆ˜í•™ ë…¼ë¦¬ì˜ ... ê¸°ì´ˆìˆ˜í•™'}

#----------

# ë°ì´í„° ì¶”ì¶œ
print(train_data_set[0]['sen'])

>>> 'ì´ ì±…ì˜ íŠ¹ì§•ã†ì½”ë”©ì˜ ê¸°ì´ˆ ê¸°ì´ˆìˆ˜í•™ ë…¼ë¦¬ì˜ ... ê¸°ì´ˆìˆ˜í•™'

#----------

# type í™•ì¸
print(train_data_set.feature)

>>> {'sen': Value(dtype='string', id=None)}

#----------

# ì €ì¥
print(train_data_set.to_csv(''))

```

<br/>
<br/>

### ğŸ¤— Transformers Trainerë¡œ ëª¨ë¸ í•™ìŠµí•˜ê¸°

í•™ìŠµì—ëŠ” ğŸ¤— Transformers Trainerë¥¼ í™œìš©í•©ë‹ˆë‹¤. ì‹¤ì œ í•™ìŠµì„ ì„¤ëª…í•˜ê¸° ì „ ğŸ¤— Transformers Trainer ì‚¬ìš©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ì‚¬ìš©ìë“¤ì„ ìœ„í•´ Trainerì— ëŒ€í•œ ê¸°ë³¸ ì‚¬ìš©ë²•ê³¼ ê¸°ëŠ¥ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ì†Œê°œí•˜ê³ ì í•©ë‹ˆë‹¤. ì§€ê¸ˆë¶€í„° ì„¤ëª…í•  ë‚´ìš©ì€ Trainerë¥¼ ì‚¬ìš©í•˜ëŠ”ë°ì— í•„ìˆ˜ë¡œ ì„¤ì •í•´ì•¼í•˜ëŠ” ê°’ì€ ì•„ë‹ˆì§€ë§Œ Trainerë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì´ë¯€ë¡œ ì²˜ìŒì€ ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì´ ìˆë‹¤ ì •ë„ë¡œ ì´í•´í•œ ë’¤ ì‹¤ì œ í•„ìš”í•  ë•Œ ì°¸ê³ í•´ì„œ í™œìš©í•˜ì‹œë©´ ë˜ê² ìŠµë‹ˆë‹¤.

#### â– TrainingArguments(ì„ íƒì‚¬í•­)

TrainingArgumentsë¥¼ í™œìš©í•˜ë©´ Trainerì— ì ìš©ë˜ëŠ” ëª¨ë“  Argumentsë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì¤‘ ìì£¼ ì“°ì´ëŠ” ëª‡ê°€ì§€ Argumentsì— ëŒ€í•´ì„œ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

- `output_dir` ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤. TrainerëŠ” ë”°ë¡œ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ê°’ìœ¼ë¡œ 500íšŒ stepì„ ìˆ˜í–‰í•˜ë©´ ëª¨ë¸ì„ ìë™ ì €ì¥í•©ë‹ˆë‹¤.

- `per_device_eval_batch_size` í‰ê°€(evaluation) ì‹œ batch 1íšŒì— í•™ìŠµí•˜ëŠ” ë¬¸ì¥ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- `per_device_train_batch_size` í•™ìŠµ(train) ì‹œ batch 1íšŒì— í•™ìŠµí•˜ëŠ” ë¬¸ì¥ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- `logging_steps` Trainerì—ì„œ stepì€ batch 1íšŒë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. logging_steps = 2ë¼ëŠ” ì˜ë¯¸ëŠ” 2íšŒì˜ stepì´ ëë‚˜ë©´ logë¥¼ print í•˜ë¼ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. logì— ëŒ€í•œ ë‚´ìš©ì€ callback í•¨ìˆ˜ë¥¼ ì„¤ëª…í•˜ë©° ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.
- `num_train_epochs` ë°ì´í„° í•™ìŠµ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- `evaluation_strategy` evaluationì„ ìˆ˜í–‰í•˜ëŠ” ì‹œê¸°ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. 'step'ê³¼ 'epoch'ê°€ ìˆìœ¼ë©° stepì€ eval_stepsì—ì„œ ì„¤ì •í•œ ë‹¨ê³„ì—ì„œ ì‹¤í–‰ë˜ë©° 'epoch'ëŠ” ë§¤ epochê°€ ì¢…ë£Œë˜ë©´ ì‹¤í–‰ë©ë‹ˆë‹¤.

> ì´ì™¸ì˜ argumentsëŠ” [TrainingArguments í˜ì´ì§€](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ë¥¼ ì°¸ê³  ë°”ëë‹ˆë‹¤.

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="test_trainer",
    per_device_eval_batch_size=8,
    per_device_train_batch_size=8,
    logging_steps=2,
    num_train_epochs=2,
    eval_steps = 100
    evaluation_strategy='steps'
)
```

<br/>

#### â– Callback(ì„ íƒì‚¬í•­)

callbackì€ í›ˆë ¨ ê³¼ì • ì¤‘ Trainer APIê°€ ì¶”ê°€ë¡œ ìˆ˜í–‰í•´ì•¼í•˜ëŠ” ë‚´ìš©ì„ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ìë©´ stepì´ ì‹œì‘í• ë•Œ ë§ˆë‹¤ í˜„ì¬ê°€ ëª‡ë²ˆì§¸ stepì¸ì§€ printí•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Callbackì€ í•„ìˆ˜ë¡œ ì„¤ì •í•´ì•¼í•˜ëŠ” í•­ëª©ì€ ì•„ë‹ˆë¯€ë¡œ í•™ìŠµ ë‚´ë¶€ì— ì–´ë–¤ ê¸°ëŠ¥ êµ¬í˜„ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ í™œìš©í•˜ë©´ ë©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•ì€ ì•„ë˜ ì½”ë“œì²˜ëŸ¼ callback classë¥¼ ì •ì˜í•˜ê³  TrainerCallbackì„ ìƒì†ë°›ì€ ë’¤ callbackì´ í•„ìš”í•œ ë‹¨ê³„ë¥¼ í•¨ìˆ˜ëª…ìœ¼ë¡œ ì •ì˜í•´ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. callback ìˆ˜í–‰ì´ ê°€ëŠ¥í•œ ë‹¨ê³„ëŠ” `on_init_end`, `on_train_begin`, `on_train_end`, `on_epoch_begin`, `on_epoch_end`, `on_step_begin`, `on_substep_end`, `on_step_end`, `on_evaluate`, `on_save`, `on_log`, `on_prediction_step` ì´ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import TrainerCallback

# custom callback ë§Œë“¤ê¸°, ì´ë•Œ TrainerCallbackì„ ìƒì† ë°›ì•„ì•¼í•¨.
class myCallback(TrainerCallback):

  # stepì´ ì‹œì‘í• ë•Œ ì•„ë˜ ì½”ë“œ ì‹¤í–‰
  def on_step_begin(self, args, state, control, logs=None, **kwargs):

      if state.global_step % args.logging_steps == 0:
        # stateëŠ” í˜„ì¬ step, epoch ë“± ì§„í–‰ ìƒíƒœì— ëŒ€í•œ ê°’ì„ ë¶ˆëŸ¬ì˜´
        # argëŠ” í›ˆë ¨ ì˜µì…˜ìœ¼ë¡œ ì„¤ì •í•œ ê°’ì„ ë¶ˆëŸ¬ì˜´.
          print("")
          print(
              f"{int(state.epoch)}ë²ˆì§¸ epoch ì§„í–‰ ì¤‘ --- {state.global_step}ë²ˆì§¸ step ê²°ê³¼"
          )
```

<br/>

ì´ë•Œ callback í•¨ìˆ˜ë¥¼ ì •ì˜í•  ë•Œì˜ argumentsëŠ” `args`, `state`, `control`, `logs`, `**kwargs`ë¥¼ ì‚¬ìš© í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°œë³„ argumentsë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ì˜µì…˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- `args`ëŠ” ì•ì„œ TrainerArgumentsì—ì„œ ì„¤ì •í•œ ê°’ì„ ë¶ˆëŸ¬ì˜¬ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ë“œ ì˜ˆì‹œì— ë‚˜ì˜¨ ë°©ë²•ì²˜ëŸ¼ args.logging_stepsì²˜ëŸ¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

- `state`ëŠ” í˜„ì¬ step, epoch ë“± ì§„í–‰ ìƒíƒœì— ëŒ€í•œ ê°’ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì„¸ë¶€ parameterëŠ” [TrainerState í˜ì´ì§€](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/callback#transformers.TrainerState)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”

- `control`ì€ í›ˆë ¨ ê³¼ì •ì„ í†µì œí•˜ëŠ” ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì„¸ë¶€ parameterëŠ” [TrainerControl í˜ì´ì§€](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/callback#transformers.TrainerControl)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”

- `logs`ëŠ” loss, learning_rate, epochë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ```python
  # logs output
  {'loss': 1.5284, 'learning_rate': 4.995452064762598e-05, 'epoch': 0.0}
  ```

- ì´ ì™¸ì—ë„ `model`, `tokenizer`, `optimizer`, `dataloader` ë“±ì„ í•¨ìˆ˜ ë‚´ë¶€ë¡œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

Callbackì„ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ê¸°ë³¸ì ìœ¼ë¡œ êµ¬í˜„ëœ Callbackì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

- `ProgressCallback` ì€ on_train_begin ë‹¨ê³„ì—ì„œ ì§„í–‰ ìƒíƒœë°”ë¥¼ callbackí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
- `PrinterCallback` ì€ on_log ìˆœì„œì—ì„œ logs ë‚´ìš©ì„ callbackí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
- `EarlyStoppingCallback` ì€ on_evaluate ìˆœì„œì—ì„œ EarlyStopì„ callbackí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
- `MLFlowCallback`ì€ logsë¥¼ mlflowë¡œ ë³´ë‚´ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  > ì¶”ê°€ì ì¸ Callbackì€ [Callbacks í˜ì´ì§€](https://huggingface.co/docs/transformers/main/en/main_classes/callback#callbacks)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

<br/>

#### â– Custom Trainer(ì„ íƒì‚¬í•­)

Callbackì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•´ì„œ ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼ Trainer ë˜í•œ ì‚¬ìš©ìì˜ í•„ìš”ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ë•Œ optimizer ì„¤ì •, loss ê³„ì‚° ë“± í•™ìŠµ ë°©ë²•ì„ ìˆ˜ì •í•˜ëŠ”ë°ë„ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, ì•„ë˜ì™€ ê°™ì´ ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì • í•˜ë‚˜í•˜ë‚˜ë¥¼ ì¶œë ¥í•´ë³´ê³  ì‹¶ì€ ê²½ìš°ì™€ ê°™ì´ ì–´ë– í•œ ë°©ë²•ìœ¼ë¡œë„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ ì˜ˆì‹œëŠ” Generatorê°€ ì‹¤ì œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì¶œë ¥í•œ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤. Trainerì˜ ë‚´ë¶€ ë§¤ì„œë“œë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•˜ë©´ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì— ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ì™€ ê°™ì€ ê¸°ëŠ¥ë„ êµ¬í˜„í•´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
0ë²ˆì§¸ epoch ì§„í–‰ ì¤‘ ------- 0ë²ˆì§¸ step ê²°ê³¼
input ë¬¸ì¥ : ì¥ ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [MASK] [MASK] [MASK] ì½ê³  ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤
output ë¬¸ì¥ : ì¥ ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [##íŒŒ] [##ë²³] [##ì„]ì„ ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤

0ë²ˆì§¸ epoch ì§„í–‰ ì¤‘ ------- 20ë²ˆì§¸ step ê²°ê³¼
input ë¬¸ì¥ : [MASK]ì´ ì¶œê°„ëœì§€ ê½¤ ë¬ë‹¤ê³  ìƒê°í•˜ëŠ”ë° ì‹¤ìŠµí•˜ëŠ”ë° ì „í˜€ [MASK]ì—†ìŠµë‹ˆë‹¤
output ë¬¸ì¥ : [ì±…]ì´ ì¶œê°„ëœì§€ ê½¤ ë¬ë‹¤ê³  ìƒê°í•˜ëŠ”ë° ì‹¤ìŠµí•˜ëŠ”ë° ì „í˜€ [ë¬¸ì œ]ì—†ìŠµë‹ˆë‹¤

```

ìœ„ì™€ ê°™ì´ ëª¨ë¸ì´ ì‹¤ì œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ ì¶”ì í•˜ê³  ì‹¶ì€ ê²½ìš° compute_loss ë§¤ì„œë“œ ë‚´ë¶€ë¥¼ ìˆ˜ì •í•´ì•¼í•©ë‹ˆë‹¤. ì´ëŠ” Trainer ë‚´ë¶€ì—ì„œ modelì´ ì‹¤ì œ ì‘ë™í•˜ëŠ” ë‹¨ê³„ê°€ compute_loss ë§¤ì„œë“œì—ì„œ ì§„í–‰ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ compute_lossëŠ” ëª¨ë¸ê³¼ input dataë¥¼ argsë¡œ ë°›ê³  ë‚´ë¶€ ë¡œì§ì— ì˜í•´ì„œ lossë¥¼ ê³„ì‚°í•œ ë‹¤ìŒ ì´ë¥¼ returní•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. compute_loss ë‚´ë¶€ëŠ” input dataì™€ modelì˜ output dataë¥¼ ë™ì‹œì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ë§¤ì„œë“œ ì´ë¯€ë¡œ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê³  ì‹¶ì€ ê²½ìš° compute_lossë¥¼ í™œìš©í•´ì•¼í•©ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œëŠ” ê¸°ì¡´ compute_lossì— ìœ„ì˜ ëª¨ë¸ ì¶œë ¥ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•œ ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ `############## ëª¨ë¸ í•™ìŠµ ê³¼ì • í™•ì¸ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€` ë‹¤ìŒì— ì¶”ê°€ëœ ë¡œì§ì´ ëª¨ë¸ ì¶œë ¥ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
class customtrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    # compute_loss í•¨ìˆ˜ ë®ì–´ì“°ê¸°
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        How the loss is computed by Trainer.

        By default, all models return the loss in the first element.

        Subclass and override for custom behavior.
        """
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None
        prds,outputs = model(inputs)
        # Save past state if it exists
        # TODO: this needs to be fixed and made cleaner later.
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            # We don't use .loss here since the model may return tuples instead of ModelOutput.
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]

        ############## ëª¨ë¸ í•™ìŠµ ê³¼ì • í™•ì¸ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€

        if self.state.global_step % self.args.logging_steps == 0:
            # self.state.global_step = í˜„ step íŒŒì•…
            # args.logging_steps = argumentì—ì„œ ì§€ì •í•œ logging_step

            # batch ì¤‘ 0 ë²ˆì§¸ ìœ„ì¹˜í•œ ë¬¸ì¥ ì„ íƒ
            num = 1
            input_id = inputs.input_ids[num].reshape(-1).data.tolist()
            output_id = outputs.logits[num].argmax(dim=-1).reshape(-1).data.tolist()
            attention_mask = inputs.attention_mask[num]

            # maskê°€ ìœ„ì¹˜í•œ idx ì¶”ì¶œí•˜ê¸°
            mask_idx = (inputs.input_ids[num] == 4).nonzero().data.reshape(-1).tolist()

            # padding ì œê±°
            input_id_without_pad = [
                input_id[i] for i in range(len(input_id)) if attention_mask[i]
            ]
            output_id_without_pad = [
                output_id[i] for i in range(len(output_id)) if attention_mask[i]
            ]

            # id to token
            # [1:-1] [CLS,SEP] ì œê±°
            inputs_tokens = self.tokenizer.convert_ids_to_tokens(input_id_without_pad)[
                1:-1
            ]
            outputs_tokens = self.tokenizer.convert_ids_to_tokens(
                output_id_without_pad
            )[1:-1]

            # output mask ë¶€ë¶„ í‘œì‹œí•˜ê¸°
            for i in mask_idx:
                # [CLS,SEP ìœ„ì¹˜ ì¡°ì •]
                outputs_tokens[i - 1] = "[" + outputs_tokens[i - 1] + "]"

            inputs_sen = self.tokenizer.convert_tokens_to_string(inputs_tokens)
            outputs_sen = self.tokenizer.convert_tokens_to_string(outputs_tokens)

            print(f"input ë¬¸ì¥ : {''.join(inputs_sen)}")
            print(f"output ë¬¸ì¥ : {''.join(outputs_sen)}")

            # input ë¬¸ì¥ : ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [MASK] [MASK] [MASK] ì½ê³  ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤
            # output ë¬¸ì¥ : ìˆ˜í•™ ê¸°í˜¸ ìˆ˜ì‹ì— ë§ì´ ì“°ì´ëŠ” ê·¸ë¦¬ìŠ¤ ì•Œ [##íŒŒ] [##ë²³] [##ì„]ì„ ì“°ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤

        return (loss, outputs) if return_outputs else loss
```

<br/>

### ëª¨ë¸ í•™ìŠµí•˜ê¸°

ì´ì œ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë°ì´í„°ì…‹, ELECTRA í•™ìŠµ êµ¬ì¡°, Trainer ì‚¬ìš©ì— í•„ìš”í•œ ì„¤ì •ê°’ë“¤ì„ ëª¨ë‘ ì„¤ëª…í–ˆìœ¼ë‹ˆ ì´ë¥¼ í™œìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì•ì„œ ìƒì„±í–ˆë˜ customtrainerì— Electra í•™ìŠµ êµ¬ì¡°(model)ê³¼ ë¶ˆëŸ¬ì™”ë˜ train_dataset, eval_dataset ê·¸ë¦¬ê³  args, callbacks, tokenizer ë“±ì„ argumentsë¡œ ë„£ìŠµë‹ˆë‹¤. ì´ë•Œ ì£¼ì˜í•  ì ì€ callback í•¨ìˆ˜ëŠ” í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ë”ë¼ë„ listì— ë„£ì–´ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.

ì•„ë˜ì™€ ê°™ì´ custom trainerë¥¼ ì •ì˜í•œ ë’¤ .train() ë§¤ì„œë“œë¥¼ ì‹¤í–‰í•˜ë©´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. í•™ìŠµì„ ì™„ë£Œí–ˆë‹¤ë©´ `ëª¨ë¸ì„ í•™ìŠµí•œ ë‹¤ìŒì€?` ë¬¸ë‹¨ì—ì„œ ì„¤ëª…í•œ ë°”ì™€ ë§ˆì°¬ê°€ì§€ë¡œ Discriminatorì—ì„œ ElectraModelì„ ì¶”ì¶œí•˜ì—¬ í™œìš©í•©ë‹ˆë‹¤.

```python
trainer = customtrainer(
    model=model.to(device),
    train_dataset=train_data_set,
    eval_dataset=validation_data_set,
    args=training_args,
    tokenizer=tokenizer,
    callbacks=[myCallback],
)

trainer.train()
```

<img src='img/interface.png'/>
