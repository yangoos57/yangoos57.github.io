---
title: "ë„ì‹í™”ë¡œ ë…¼ë¬¸ ì´í•´í•˜ê¸° : Transformer"
category: "DeepLearning"
date: "2022-11-11"
thumbnail: "./img/transformer.png"
desc: pytorchë¥¼ í™œìš©í•´ Transformer ë…¼ë¬¸ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©° ëª¨ë¸ì˜ ìƒì„¸ ì‘ë™ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ì˜€ë‹¤. êµ¬í˜„í•œ Transformer ëª¨ë¸ì„ í™œìš©í•´ í•™ìŠµê³¼ í‰ê°€í•˜ëŠ” ê³¼ì •ì„ ê²½í—˜í•  ìˆ˜ ìˆë„ë¡ íŠœí† ë¦¬ì–¼ì„ ì œì‘í–ˆìœ¼ë©°, íŠœí† ë¦¬ì–¼ì„ í†µí•´ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì–´ë–»ê²Œ ë°ì´í„°ê°€ íë¥´ëŠ”ì§€, ì–´ë– í•œ ê³¼ì •ì„ ê±°ì³ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ê²°ê³¼ë¬¼ì„ ì‚°ì¶œí•˜ëŠ”ì§€ë¥¼ ì´í•´í•  ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì— í¬í•¨ëœ Transformerì˜ ë„ì‹í™” ê·¸ë¦¼ì„ í™œìš©í•´ Transformer êµ¬ì¡° ì „ë°˜ì— ëŒ€í•œ ì´í•´ì— ë„ì›€ì„ ì¤€ë‹¤.
---

### ë“¤ì–´ê°€ë©°

ì´ ê¸€ì€ Transformer ë…¼ë¬¸ì˜ êµ¬ì¡°ë¥¼ pytorchë¡œ êµ¬í˜„í•´ë³´ë©° ì„¸ë¶€ì ì¸ ëª¨ë¸ ì‘ë™ ë°©ì‹ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

> Transformer ëª¨ë¸ì„ ì‹¤ì œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ë¶„ë“¤ì€ [í•™ìŠµ íŠœí† ë¦¬ì–¼](https://github.com/yangoos57/Transformer_from_scratch)ì„ ì°¸ê³ ë°”ëë‹ˆë‹¤. ë§í¬ì— ì—°ê²°ëœ íŠœí† ë¦¬ì–¼ì€ ì•„ë˜ì™€ ê°™ì´ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì‹œê°í™”í•˜ì—¬ Transformer ëª¨ë¸ì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    1ë²ˆì§¸ epoch ì‹¤í–‰
    ------------------------------

    Dataset is "training"

    200ë²ˆì§¸ batchì— ìˆëŠ” 0ë²ˆì§¸ ë¬¸ì¥ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸

    src(í”„ë‘ìŠ¤ì–´) :  Un homme en uniforme orange pose au milieu d' une rue .
    prd(ì˜ì–´ ì˜ˆì¸¡) :  A man in a suit shirt is in front air of the building . <eos> . . . . . . . <eos> . . <eos> . . . <eos>
    trg(ì‹¤ì œ ì •ë‹µ) :  A man in an orange uniform poses in the middle of a street .

<br/>

### Transformer êµ¬ì¡°

ì´ ê¸€ì€ ì•„ë˜ì˜ ë„ì‹í™”ë¥¼ ë¶€ë¶„í™”í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤. ë¶€ë¶„í™” ëœ ê¸°ëŠ¥ì„ pytorchë¥¼ ì‚¬ìš©í•´ í•˜ë‚˜ì”© êµ¬í˜„í•´ ë‚˜ê°€ë©´ì„œ Transformerì˜ ì„¸ë¶€ ì‘ë™ì›ë¦¬ë¥¼ ì´í•´í•˜ê³ ì í•©ë‹ˆë‹¤.

<img alt='img11' src='./img/img11.png'>

### Transformer íƒ„ìƒ ë°°ê²½

Transformer ëª¨ë¸ì€ RNNì„ ì‚¬ìš©í•´ Seq2seq(encoder+decoder êµ¬ì¡°)ì„ êµ¬í˜„í–ˆì„ ë•Œ ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì •í™•ë„ê°€ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëìŠµë‹ˆë‹¤. RNN êµ¬ì¡°ëŠ” ë¬¸ì¥ ë‚´ ë‹¨ì–´ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë’·ë‹¨ì—ì„œ ë¶€ë¶„ì—ì„œ ì‹œì‘ ë¶€ë¶„ì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ì°¸ì¡°í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê³ ìí•˜ëŠ” ë‹¤ì–‘í•œ ì‹œë„ê°€ ìˆì—ˆê³  ê·¸ ì¤‘ attentionì´ë¼ëŠ” ê°œë…ì„ ë„ì…í•´ RNNì„ ë„ì™€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ê°œë°œ ë˜ì—ˆìŠµë‹ˆë‹¤. RNNì´ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì°¸ê³ í•˜ëŠ” ê²½ìš° ê³¼ë¶€í™”ë¥¼ ìœ ë°œí•˜ê³  ìˆìœ¼ë‹ˆ ë‹¨ì–´ë³„ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•œë‹¤ë©´ ê³¼ë¶€í™”ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆë‹¤ íŒë‹¨í•´ Attentionì´ ê°œë°œëœ ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‹œë„ëŠ” ì„±ê³µì ì´ì—ˆê³  ì´ì— ë” ë‚˜ì•„ê°€ RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  attentionë§Œì„ ì‚¬ìš©í•´ Seq2Seq êµ¬ì¡°ë¥¼ êµ¬í˜„í•œ Transformer ëª¨ë¸ì´ íƒ„ìƒí–ˆìŠµë‹ˆë‹¤. Transformer ëª¨ë¸ì€ ë¬¸ì¥ì˜ ì‹œì‘ ë‹¨ì–´ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ ì™¸ì—ë„ RNNì˜ ë‹¤ë¥¸ ë‹¨ì  í•˜ë‚˜ë¥¼ ê·¹ë³µí–ˆëŠ”ë°, attentionë§Œìœ¼ë¡œ seq2seq ëª¨ë¸ì„ êµ¬í˜„í•¨ìœ¼ë¡œì¨ RNNì—ì„œëŠ” í•  ìˆ˜ ì—†ëŠ” ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•´ì¡Œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¥ì ìœ¼ë¡œ ì¸í•´ TransformerëŠ” ì‚¬ì‹¤ìƒ NLP ëª¨ë¸ì˜ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ê²Œ ëìŠµë‹ˆë‹¤.

> í˜„ì¬ í™œìš©ë˜ê³  ìˆëŠ” Bert ëª¨ë¸ì´ë‚˜ GPT ëª¨ë¸ì€ ëª¨ë‘ Transformerì—ì„œ íŒŒìƒëœ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Transformerë¥¼ ì´í•´í•œë‹¤ë©´ NLP ëª¨ë¸ì˜ í•µì‹¬ ì¶•ì¸ Bert, GPT ë˜í•œ ìì—°ìŠ¤ëŸ½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Encoder

Input êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ê¸° ì „ì— Transformerì˜ í•µì‹¬ì¸ Encoder êµ¬ì¡°ì™€ Decoder êµ¬ì¡°ë¥¼ ë¨¼ì € ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Encoder ë‚´ë¶€ êµ¬ì¡°ëŠ” Multi-head Attentionì™€ Feed Forwardë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ê°œë³„ ê³¼ì •ì„ ë°˜ë³µ ìˆ˜í–‰í•˜ë‹¤ë³´ë©´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing)ì„ ë§‰ê¸°ìœ„í•´ Add & Normì„ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë¶€ì˜ layerë“¤ì„ ìˆ˜í–‰í•œ Input DataëŠ” ë‹¤ì‹œ ìœ—ë‹¨ì˜ Encoderì˜ Input Dataë¡œ í™œìš©ë˜ê²Œ ë˜ë©°, ì´ëŸ¬í•œ ê³¼ì •ì„ ì—¬ëŸ¬ì°¨ë¡€ ë°˜ë³µí•œ Input DataëŠ” ê¶ê·¹ì ìœ¼ë¡œ Decoder êµ¬ì¡°ì˜ Contextë¡œì„œ í™œìš©ë©ë‹ˆë‹¤. Contextì— ëŒ€í•œ ì„¤ëª…ì€ Decoder ë¬¸ë‹¨ì—ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

<img alt='encoder_block_0' src='./img/encoder_block_0.png'>

## Multi-head Attention

Encoder êµ¬ì¡°ì˜ ì²«ë²ˆì§¸ êµ¬ì„±ìš”ì†Œì¸ Multi-head-Attentionì€ ì—¬ëŸ¬ ê°œì˜ Self-Attentionì„ í•©ì¹œ êµ¬ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë•Œ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  Attention ê°œìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ” ì¸ì(args)ë¥¼ Headë¼ê³  í•˜ë©° ë…¼ë¬¸ì—ì„œëŠ” 8ê°œë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. Multi-head Attentionì€ Self-Attention 8ê°œë¥¼ ì—°ê²°í•œ ê²ƒì— ë¶ˆê³¼í•œ ê°œë…ì´ë¯€ë¡œ Self-Attentionì„ ì´í•´í•˜ëŠ” ê²ƒì´ Multi-head Attentionì„ ì´í•´í•˜ëŠ” ê²ƒì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img alt='encoder_block_1' src='./img/encoder_block_1.png'>

### Self-Attentionê³¼ Cross-Attention

Attentionì€ Self-Attention, Cross-Attentionìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Self-Attentionì€ í•˜ë‚˜ì˜ ë¬¸ì¥ ë‚´ë¶€ì— ìˆëŠ” Token ê°„ ì¤‘ìš”ë„ë¥¼ íŒŒì•…í•˜ëŠ” ë°©ë²•ì´ë¼ë©´ Cross-Attentionì€ ë¬¸ì¥ê³¼ ë¬¸ì¥ ê°„ì˜ Tokenì„ ë¹„êµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. TransformerëŠ” ë‘ ì¢…ë¥˜ì˜ Attetionì„ ê°ê° Encoder, Decoder ë‹¨ì—ì„œ ì‚¬ìš©í•˜ëŠ”ë°, Self-Attetnionì€ Encoder ë‚´ë¶€ì˜ Multi-head attentionì—ì„œ, Cross-Attentionì€ Decoder ë‚´ë¶€ì˜ Multi-head Attentionì—ì„œ í™œìš©ë©ë‹ˆë‹¤.

### Multi-head Attentionì˜ Parameter ì†Œê°œ

ì•ì„  ì„¤ëª…ì—ì„œ Multi-head Attentionì€ ì—¬ëŸ¬ ê°œì˜ Self-Attentionì„ í•©ì¹œ ê²ƒì„ ì˜ë¯¸í•œë‹¤ê³  ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ë³‘ë ¬ ì—°ì‚°ìœ¼ë¡œ í•œë²ˆì— nê°œì˜ Self-Attentionì„ ê³„ì‚°í•˜ê³  ì´ë¥¼ í•©ì¹˜ë©´(concatenation) Mulit-head Atentionì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Multi-head Attentionì—ì„œ ì‚¬ìš©ë˜ëŠ” ParameterëŠ” Self-Attentionì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•˜ëŠ” Attention Headì™€ ê°œë³„ Self-Attentionì˜ embedding_sizeì…ë‹ˆë‹¤.

ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ Attention Headì˜ ê¸°ë³¸ê°’ì€ 8ì´ë©° Self-Attentionì˜ embedding_sizeëŠ” 64ì…ë‹ˆë‹¤. Self-Attentionì˜ embedding_sizeë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€ ëª¨ë¸ì˜ embedding_sizeë¥¼ Attention Headë¡œ ë‚˜ëˆˆ ê°’ìœ¼ë¡œ, ë…¼ë¬¸ì—ì„œ ì„¤ì •í•œ ëª¨ë¸ì˜ ê¸°ë³¸ embedding_size ì¸ 512ì™€ Attention headì˜ ìˆ˜ì¸ 8ì„ ë‚˜ëˆ„ì–´ 64ë¼ë¥¸ ê²°ê³¼ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.

### Multi-head Attention êµ¬í•˜ê¸°

Multi-head Attentionì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì„¤ëª…ì„ ë§ˆì³¤ìœ¼ë‹ˆ Attentionì„ êµ¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. ì´ë²ˆ ë¬¸ë‹¨ì—ì„œ ì„¤ëª…í•˜ëŠ” Attention êµ¬í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒ ë¬¸ë‹¨ì—ì„œ ê·¸ëŒ€ë¡œ ì½”ë“œë¡œ êµ¬í˜„ë˜ë‹ˆ ì°¸ê³ í•˜ì—¬ ì½ìœ¼ë©´ Attentionì— ëŒ€í•´ ë”ìš± ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### scaled dot product Attentionì„ êµ¬í•˜ëŠ” 6ë‹¨ê³„

**Attention ê³µì‹**

$Attention(Q,K,V)= \mathrm{softmax} \left( \frac{Q K^\text{T}}{\sqrt{d_k}} \right) V$

**Q** : Query Vector,&nbsp; **K** : Key Vector,&nbsp; **V** : Value Vector,&nbsp; **${d_k} $** : Self-Attention í¬ê¸°(64)

#### â– Step 1: Create three vectors(Q,K,V) from each of the encoderâ€™s input vectors

Attentionì´ ë¬¸ì¥ ë‚´ í† í°ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì€ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ëª¨ë°©í•œ ê²ƒì´ë¼ê³  í•©ë‹ˆë‹¤. ì˜ˆë¡œë“¤ì–´ ì‚¬ìš©ìê°€ êµ¬ê¸€ ê²€ìƒ‰ì„ í•œë‹¤ë©´, ê²€ìƒ‰ì— í™œìš©í•œ ë‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ê³¼ ê°™ì€ ê¸°ëŠ¥ì€ queryì— ëŒ€ì‘ë˜ê³ , í˜ì´ì§€ ì œëª©, ì„¤ëª…, íƒœê·¸ì™€ ê°™ì´ ê²€ìƒ‰ì— í™œìš©ë˜ëŠ” indexëŠ” keyì— ëŒ€ì‘ë˜ë©°, ê²€ìƒ‰ì„ í†µí•´ ì–»ì€ ê²°ê³¼ëŠ” Valueì— ëŒ€ì‘ë©ë‹ˆë‹¤.

Attentionì„ êµ¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ Query, Key, ValueëŠ” Encoderì˜ Input Dataë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤. Encoderì˜ Input Dataì— ëŒ€í•œ tensor ì°¨ì›ì€ (N_Batch, token_len, embedding_size) ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì°¨ì›ì„ ê°€ì§„ Input DataëŠ” (N_Batch, token_len, head, d_k)ë¡œ reshape ë©ë‹ˆë‹¤.(head \* d_k = embedding size) ì´ë•Œ Encoderì˜ Attentionì€ Self-Attentionì´ë¯€ë¡œ Query, Key, Value ëª¨ë‘ Input Dataë¡œ ë¶€í„° ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ Query, Key, ValueëŠ” ì´ë¦„ë§Œ ê°™ì„ ë¿ ë‚´ìš©ì€ ë™ì¼í•©ë‹ˆë‹¤.

> Input dataì˜ Tensor ì°¨ì›ì´ (N_Batch, token_len, embedding_size)ë˜ëŠ” ì´ìœ ëŠ” Transformerê°€ í•œ ë²ˆì— ì—¬ëŸ¬ê°œì˜ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ê³ (N_batch) ê°œë³„ ë¬¸ì¥ì€ ì—¬ëŸ¬ ê°œì˜ tokenì„ ê°–ê³  ìˆìœ¼ë©°(token_len), ê°œë³„ ë‹¨ì–´ëŠ” embedding_sizeë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì˜ˆë¡œë“¤ì–´ 8ê°œì˜ ë¬¸ì¥ì„ ë³‘ë ¬ë¡œ í•™ìŠµí•˜ê³ , 8ê°œ ë¬¸ì¥ ì¤‘ ìµœëŒ€ token ê°œìˆ˜ê°€ 20ì´ê³ , ë‹¨ì–´ì˜ embedding_sizeë¥¼ 512ë¡œ í‘œí˜„í•œë‹¤ë©´ Input dataëŠ” (8,20,512)ì˜ ì°¨ì›ì„ ê°€ì§„ Tensorê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

<br/>

#### â– Step 2: Calculate a score

ì•ì„œ Attentionì´ ë‹¨ì–´ê°„ ì¤‘ìš”ë„ë¥¼ íŒŒì•…í•˜ëŠ” ë°©ì‹ì€ ê²€ìƒ‰ ì—”ì§„ì„ ëª¨ë°©í–ˆë‹¤ê³  ë§ì”€ë“œë ¸ìŠµë‹ˆë‹¤. Scoreì„ êµ¬í•˜ëŠ” ê³¼ì •ì€ ê²€ìƒ‰ì—”ì§„ì´ queryì™€ indexë¥¼ ë¹„êµí•´ ì–´ë–¤ ë‚´ìš©ì„ ìˆ˜ì§‘í• ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê³¼ì •ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. Attention Token í•˜ë‚˜ì™€ ë¬¸ì¥ ì „ì²´ì˜ ì—°ê´€ì„±ì„ ë¹„êµí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ Scoreì„ êµ¬í•©ë‹ˆë‹¤. ì´ë•Œ ë¬¸ì¥ ë‚´ ëª¨ë“  í† í°ì´ í•œë²ˆì”© Query ì—­í• ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ëª¨ë“  Tokenì— ëŒ€í•œ Scoreì„ êµ¬í•©ë‹ˆë‹¤.

ë¬¸ì¥ `The animal didn't cross the street because it was too tired.`ì„ ì˜ˆë¡œ ë“¤ì–´ Scoreë¥¼ ê³„ì‚°í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì‹¤ì œ ì—°ì‚°ì€ (N_batch, token_len, head, d_k) ì°¨ì›ìœ¼ë¡œ ê³„ì‚°í•´ì•¼í•˜ì§€ë§Œ ì—°ì‚°ì„ ê°„ë‹¨íˆ í•˜ê³  ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ token_lenê³¼ d_kë§Œ í™œìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Queryì™€ KeyëŠ” (token_len, d_k)ì˜ ì°¨ì›ì„ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤. ì´ì œ queryì™€ keyë¥¼ í†µí•´ scoreë¥¼ êµ¬í•˜ë©´ scoreì˜ ì°¨ì›ì€ (token_len \* token_len)ì´ ë©ë‹ˆë‹¤. scoreì˜ ê°œë³„ ê°’ì€ ë‹¨ì–´ì™€ ë‹¨ì–´ ê°„ scoreì´ ë˜ëŠ”ë°, scoreì˜ (0,0)ì˜ ê²½ìš° theì™€ theì˜ score ê°’ì´ ë©ë‹ˆë‹¤.

  <img alt='attention_score' src='./img/attention_score.png'>

<br/>

#### â– Step 3: Divide the score by $\sqrt{d_k}$

scoreì˜ ì ˆëŒ€ì ì¸ í¬ê¸°ë¥¼ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•´ Scoreì„ $\sqrt{d_k}$ ë‚˜ëˆ•ë‹ˆë‹¤. Scoreì„ $\sqrt{d_k}$ë¡œ ë‚˜ëˆ ì•¼ í•˜ëŠ” ì´ë¡ ì ì¸ ê·¼ê±°ëŠ” ì—†ê³  ê²½í—˜ì ìœ¼ë¡œ ë´¤ì„ ë•Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤.

<br/>

#### â– Step 4: Pass the result through a softmax operation

ì´ì œ scoreì„ 0 ~ 1 ì‚¬ì´ë¡œ ì¡°ì •í•˜ê¸° ìœ„í•´ softmaxë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¡°ì •ì´ ëë‚œ scoreë“¤ì€ queryì™€ ì—°ê´€ì„± ë†’ì€ keyì¼ìˆ˜ë¡ 1ì— ê·¼ì ‘í•œ ê°’ì„, ë‚®ì„ìˆ˜ë¡ 0ì— ê·¼ì ‘í•œ ê°’ì„ ë¶€ì—¬ë°›ê²Œ ë©ë‹ˆë‹¤.

<br/>

#### â– Step 5: Multiply each value vector by the softmax score

ê°œë³„ scoreëŠ” 0 ~ 1 ì‚¬ì´ì˜ ê°’ì„ ê°–ê³  ìˆìŠµë‹ˆë‹¤. scoreê³¼ valueë¥¼ ê³±í•´ ì–»ì€ ê²°ê³¼ì¸ Attentionì€ ë‹¨ì–´ ë³„ ê°€ì¤‘ì¹˜ê°€ ë°˜ì˜ëœ embeddingì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  <img alt='attention_finish' src='./img/attention_finish.png'>

  <br/>

#### â– Step 6 : Sum up the weighted value vector which produces the output of the self-attention layer at this position

ì§€ê¸ˆê¹Œì§€ Step 1 ~ 5ë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ëŠ” (token_len, d_k) ì°¨ì›ì˜ Self-Attentionì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë²ˆ ì–¸ê¸‰í–ˆë“¯ Self-Attentionì„ ë³‘í•©í•œ ê²ƒì´ Multi-head Attentionì´ ë¯€ë¡œ ì•ì„  ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ 7ê°œì˜ Self-Attentionì„ êµ¬í•œ ë’¤ ë³‘í•©(concatenation)í•˜ë©´ Multi-head Attentionì´ ë©ë‹ˆë‹¤.

Query, Key, ValueëŠ” Input dataë¡œ ë§Œë“¤ì–´ì§„ë‹¤ê³  ì„¤ëª…í•œ ë°” ìˆìŠµë‹ˆë‹¤. (8,20,512)ì˜ ì°¨ì›ì„ ê°€ì§„ Input Dataë¥¼ Query, Key, Valueë¡œ ë§Œë“¤ë©´ ê°ê°ì€ (8,20,8,64)ê°€ ë©ë‹ˆë‹¤. Step 1 ~ 5ì—ì„œ ì„¤ëª…í•œ ì˜ˆì‹œëŠ” ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ (8,20,8,64)ì°¨ì›ì˜ ì˜ˆì‹œë¥¼ (20,64)ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ë³€ê²½í•œ ê²ƒì…ë‹ˆë‹¤. Step 1 - 5ì˜ ì‹¤ì œ ê³¼ì •ì— ì‚¬ìš©ëœ ë°ì´í„° ì°¨ì›ì€ ì‹¤ì œë¡œëŠ” (1,20,1,64)ì…ë‹ˆë‹¤. ì´ëŠ” 8ê°œ Self-Attention ì¤‘ ê³ ì‘ í•˜ë‚˜ì˜ Self-Attentionì„ êµ¬í•œ ê²ƒì— ë¶ˆê³¼í•˜ë¯€ë¡œ ë‚˜ë¨¸ì§€ 7ê°œì˜ Self-Attenionì„ êµ¬í•œ ë‹¤ìŒ ë³‘í•©í•´ì•¼ `The animal didn't cross the street because it was too tired.`ì˜ Multi-head Attentionì„ êµ¬í•œ ê²ƒì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë¬¼ë¡  ì‹¤ì œ Multi-head-Attention ì—°ì‚°ì€ ë³‘ë ¬ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.)

<br/>

### Multi-head Attention êµ¬í˜„í•˜ê¸°

```python
import torch
import torch.nn as nn

class selfAttention(nn.Module):
    def __init__(self, embed_size, heads) -> None:
        """
        config ì°¸ê³ 
        embed_size(=512) : embedding ì°¨ì›
        heads(=8) : Attention ê°œìˆ˜
        """
        super().__init__()
        self.embed_size = embed_size  # 512
        self.heads = heads  # 8
        self.head_dim = embed_size // heads  # ê°œë³„ attentionì˜ embed_size(=64)

        # Query, Key, Value
        self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)  # 64 => 64
        self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)  # 64 => 64
        self.value = nn.Linear(self.head_dim, self.head_dim, bias=False)  # 64 => 64

        # 8ê°œ attention => 1ê°œì˜ attentionìœ¼ë¡œ ìƒì„±
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)  # 8 * 64 => 512

    def forward(self, value, key, query, mask):
        """
        query, key, value size : (N, seq_len, embed_size)
        - N_batch = ë¬¸ì¥ ê°œìˆ˜(=batch_size)
        - seq_len : í›ˆë ¨ ë¬¸ì¥ ë‚´ ìµœëŒ€ token ê°œìˆ˜
        - embed_size : embedding ì°¨ì›
        """

        N_batch = query.shape[0]  # ì´ ë¬¸ì¥ ê°œìˆ˜
        value_len = value.shape[1]  # token ê°œìˆ˜
        key_len = key.shape[1]  # token ê°œìˆ˜
        query_len = query.shape[1]  # token ê°œìˆ˜

        # n : batch_size(=128)
        # h : heads(=8)
        # value,key,query_len, : token_len
        # d_k : embed_size/h(=64)

        value = value.reshape(
            N_batch, self.heads, value_len, self.head_dim
        )  # (n, h, value_len, d_k)
        key = key.reshape(
            N_batch, self.heads, key_len, self.head_dim
        )  # (n x h x key_len x d_k)
        query = query.reshape(
            N_batch, self.heads, query_len, self.head_dim
        )  # (n x h x query_len x d_k)

        # Q,K,V êµ¬í•˜ê¸°
        V = self.value(value)
        K = self.key(key)
        Q = self.query(query)

        # score = Q dot K^T
        score = torch.matmul(Q, K.transpose(-2, -1))
        # query shape : (n, h, query_len, d_k)
        # transposed key shape : (n, h, d_k, key_len)
        # score shape : (n, h, query_len, key_len)

        if mask is not None:
            score = score.masked_fill(mask == 0, float("-1e20"))
            """
            mask = 0 ì¸ ê²½ìš° -inf(= -1e20) ëŒ€ì…
            softmax ê³„ì‚°ì‹œ -infì¸ ë¶€ë¶„ì€ 0ì´ ë¨.
            """

        # attention ì •ì˜

        # d_kë¡œ ë‚˜ëˆˆ ë’¤ => softmax
        d_k = self.embed_size ** (1 / 2)
        softmax_score = torch.softmax(score / d_k, dim=3)
        # softmax_score shape : (n, h, query_len, key_len)

        # softmax * Value => attention í†µí•©ì„ ìœ„í•œ reshape
        out = torch.matmul(softmax_score, V).reshape(
            N_batch, query_len, self.heads * self.head_dim
        )
        # softmax_score shape : (n, h, query_len, key_len)
        # value shape : (n, h, value_len, d_k)
        # (key_len = value_len ì´ë¯€ë¡œ)
        # out shape : (n, h, query_len, d_k)
        # reshape out : (n, query_len, h, d_k)

        # concat all heads
        out = self.fc_out(out)
        # concat out : (n, query_len, embed_size)

        return out

```

### Add & Normalization

<img alt='encoder_block_2' src='./img/encoder_block_2.png'>

addë¥¼ í•˜ëŠ” ì´ìœ ëŠ” gradient vanishing(gradient descentê°€ 0ì´ ë˜ëŠ” í˜„ìƒ)ì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ë©° ì¼ë°˜ì ìœ¼ë¡œ residual connectionë¼ëŠ” ìš©ì–´ë¡œ ë¶ˆë¦½ë‹ˆë‹¤. residual connectionì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- y = f(x) + x (xëŠ” input f(x)ëŠ” layerì˜ ouput)

Normalizationì„ í•˜ëŠ” ì´ìœ ëŠ” gradientê°€ explodingí•˜ê±°ë‚˜ vanishingí•˜ëŠ” ë¬¸ì œë¥¼ ì™„í™”ì‹œí‚¤ê³  gradient ê°’ì´ ì•ˆì •ì ì¸ ê°’ì„ ê°€ì§€ê²Œ í•˜ì—¬ ë¹ ë¥¸ í•™ìŠµì„ ë³´ì¥ë°›ê¸° ìœ„í•¨ì´ë¼ê³  í•©ë‹ˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ Dropoutì„ ìˆ˜í–‰í•´ ê°œë³„ Nodeê°€ ê³¨ê³ ë£¨ í•™ìŠµë˜ë„ë¡ í•©ë‹ˆë‹¤.

  <img alt='add_norm' src='./img/add_norm.png'>

### position-wise Feed Forward Neural Network(FFNN)

<img alt='encoder_block_3' src='./img/encoder_block_3.png'>

- TransformerëŠ” ReLUë¥¼ activation functionìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. FFNNì€ Linear(512d,2048d)-> ReLU(2048d) -> Linear(2048d, 512d)ì˜ Position-Wiseí•œ êµ¬ì¡°ë¡œ ì´ë¤„ì§‘ë‹ˆë‹¤.

  <img alt='img10' src='./img/img10.png'>
  <figcaption>Jay Alammar, The Illustrated Transformer</figcaption>

### Encoder Block êµ¬í˜„

<img alt='encoder_block_0' src='./img/encoder_block_0.png'>

```python
class EncoderBlock(nn.Module) :
    class EncoderBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:
        """
        config ì°¸ê³ 
        embed_size(=512) : embedding ì°¨ì›
        heads(=8) : Attention ê°œìˆ˜
        dropout(=0.1): Node í•™ìŠµ ë¹„ìœ¨
        forward_expansion(=2) : FFNNì˜ ì°¨ì›ì„ ì–¼ë§ˆë‚˜ ëŠ˜ë¦´ ê²ƒì¸ì§€ ê²°ì •,
                                forward_expension * embed_size(2*512 = 1024)
        """
        super().__init__()
        # Attention ì •ì˜
        self.attention = selfAttention(embed_size, heads)

        ### Norm & Feed Forward
        self.norm1 = nn.LayerNorm(embed_size)  # 512
        self.norm2 = nn.LayerNorm(embed_size)  # 512

        self.feed_forawrd = nn.Sequential(
            # 512 => 1024
            nn.Linear(embed_size, forward_expansion * embed_size),
            # ReLU ì—°ì‚°
            nn.ReLU(),
            # 1024 => 512
            nn.Linear(forward_expansion * embed_size, embed_size),
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):

        # self Attention
        attention = self.attention(value, key, query, mask)
        # Add & Normalization
        x = self.dropout(self.norm1(attention + query))
        # Feed_Forward
        forward = self.feed_forawrd(x)
        # Add & Normalization
        out = self.dropout(self.norm2(forward + x))
        return out
```

## Input Embeddingê³¼ Positional Encoding

- Input Embedding + Positional encodingì€ ì²«ë²ˆì§¸ Encoder Blockì˜ Input Dataë¡œ í™œìš©ë©ë‹ˆë‹¤.

### Input Embedding

<img alt='input_0' src='./img/input_0.png'>

- Embeddingì€ ë²¡í„° ê³µê°„ ë‚´ ë‹¨ì–´ì˜ positionì„ íŠ¹ì • ì°¨ì› ë‚´ì— í‘œí˜„í•˜ì—¬ ë‹¨ì–´ì˜ ìœ ì‚¬ë„, ì—°ê´€ì„± ë“±ì„ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Transfromer ë…¼ë¬¸ì—ì„œëŠ” 512ì°¨ì›ì„ ê¸°ë³¸ Embeddingìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Positional Embedding

<img alt='input_1' src='./img/input_1.png'>

RNN ë°©ì‹ì˜ ëª¨ë¸ì€ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ìˆœì„œì— ëŒ€í•œ íŒ¨í„´ ë˜í•œ í•™ìŠµ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë³‘ë ¬ì—°ì‚°ì´ ê°€ëŠ¥í•œ Transformerì—ì„œëŠ” ë‹¨ì–´ ìˆœì„œë¥¼ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” íŒ¨í„´ì„ í•™ìŠµí•  ê¸°íšŒê°€ ì‚¬ë¼ì¡Œê³  ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Input Embeddingì— ë‹¨ì–´ ê°„ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” Positional Embeddingì„ ë”í•˜ì—¬ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### Padì— ëŒ€í•´ Maskingí•˜ê¸°

- Padë¼ëŠ” ê°œë…ì€ Encoderì™€ Decoderì— ê°ê° í•œ ë²ˆì”© ë“±ì¥í•©ë‹ˆë‹¤. ë°©ë²•ì€ ë™ì¼í•˜ë‚˜ í™œìš© ëª©ì ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. Encoderì—ì„œ Maskingì„ ì“°ëŠ” ì´ìœ ëŠ” ëª¨ë“  ëª¨ë“  ë¬¸ì¥ì˜ í† í° ê°œìˆ˜ë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ pad í† í°ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 8ê°œì˜ ë¬¸ì¥ì„ í•œ ë²ˆì— í•™ìŠµí•œë‹¤ê³  ìƒê°í•  ë•Œ 8ê°œ ë¬¸ì¥ì˜ ê°œë³„ í† í°ì˜ ê°œìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œëŠ” ëª¨ë‘ ìƒì´í•©ë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì¥ì€ 20ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ë˜ê³  ì–´ë–¤ ë¬¸ì¥ì€ 8ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ë  ìˆ˜ ë„ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ë¬¸ì¥ë³„ë¡œ ê°ê° í† í° ê°œìˆ˜ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì€ í•˜ë‚˜ë¡œ í†µì¼ í•´ì•¼í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì‚ì£½ë¹¼ì£½í•œ í–‰ë ¬ì€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë‹ˆê¹Œìš”.

Maskingì€ Pad í† í°ì´ Attention ê³„ì‚°ì— í™œìš©ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ”ë° ëª©í‘œê°€ ìˆìŠµë‹ˆë‹¤. Padì— -infë¥¼ ë¶€ì—¬í•˜ë©´ Attention ê³„ì‚° ê³¼ì • ì¤‘ Step 4 softmaxë¥¼ í•˜ëŠ” ë‹¨ê³„ì—ì„œ ëª¨ë‘ 0ì´ ë˜ë¯€ë¡œ Value ê³„ì‚° ì‹œ í•´ë‹¹ Padì˜ ê°’ì´ ëª¨ë‘ 0 ì´ ë©ë‹ˆë‹¤.

````python
  def make_pad_mask(self, query, key):
      """
      Multi-head attention pad í•¨ìˆ˜
      """
      len_query, len_key = query.size(1), key.size(1)

      key = key.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)
      # (batch_size x 1 x 1 x src_token_len) 4

      key = key.repeat(1, 1, len_query, 1)
      # (batch_size x 1 x len_query x src_token_len) 4

      query = query.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)
      # (batch_size x 1 x src_token_len x 1) 4

      query = query.repeat(1, 1, 1, len_key)
      # (batch_size x 1 x src_token_len x src_token_len) 4

      mask = key & query
      return mask

    ```
````

### Encoder êµ¬í˜„(= Encoder x num_layers)

<img alt='encoder' src='./img/encoder.png'>

```python
class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        max_length,
        device,
    ) -> None:
        """
        config ì°¸ê³ 
        src_vocab_size(=11509) : input vocab ê°œìˆ˜
        embed_size(=512) : embedding ì°¨ì›
        num_layers(=3) : Encoder Block ê°œìˆ˜
        heads(=8) : Attention ê°œìˆ˜
        device : cpu;
        forward_expansion(=2) : FFNNì˜ ì°¨ì›ì„ ì–¼ë§ˆë‚˜ ëŠ˜ë¦´ ê²ƒì¸ì§€ ê²°ì •,
                                forward_expension * embed_size(2*512 = 1024)
        dropout(=0.1): Node í•™ìŠµ ë¹„ìœ¨
        max_length : batch ë¬¸ì¥ ë‚´ ìµœëŒ€ token ê°œìˆ˜(src_token_len)
        """
        super().__init__()
        self.embed_size = embed_size
        self.device = device

        # input + positional_embeding
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # (11509, 512) 2

        # positional embedding
        pos_embed = torch.zeros(max_length, embed_size)  # (src_token_len, 512) 2
        pos_embed.requires_grad = False
        position = torch.arange(0, max_length).float().unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size)
        )
        pos_embed[:, 0::2] = torch.sin(position * div_term)
        pos_embed[:, 1::2] = torch.cos(position * div_term)
        self.pos_embed = pos_embed.unsqueeze(0).to(device)  # (1, src_token_len, 512) 3

        # Encoder Layer êµ¬í˜„
        self.layers = nn.ModuleList(
            [
                EncoderBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )
        # dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        _, seq_len = x.size()  # (n, src_token_len) 2
        # n : batch_size(=128)
        # src_token_len : batch ë‚´ ë¬¸ì¥ ì¤‘ ìµœëŒ€ í† í° ê°œìˆ˜

        pos_embed = self.pos_embed[:, :seq_len, :]
        # (1, src_token_len, embed_size) 3

        out = self.dropout(self.word_embedding(x) + pos_embed)
        # (n, src_token_len, embed_size) 3

        for layer in self.layers:
            # Q,K,V,mask
            out = layer(out, out, out, mask)
        return out
```

## Decoder

Decoder êµ¬ì¡°ì™€ Encoder êµ¬ì¡°ëŠ” ê±°ì˜ ìœ ì‚¬í•©ë‹ˆë‹¤. ì°¨ì´ê°€ ìˆë‹¤ë©´ Encoderì— ìˆëŠ” Layer ì™¸ì—ë„ Masked Multi-head Attentionì´ ì¶”ê°€ë˜ë©°, Multi-head Attentionì—ì„œ encoder ê³¼ì •ì—ì„œ í™•ë³´í•œ ìµœì¢… ê²°ê³¼ê°’(Context)ì„ Keyì™€ Valueë¡œ í™œìš©í•©ë‹ˆë‹¤. Decoder ë˜í•œ Encoderì™€ ë§ˆì°¬ê°€ì§€ë¡œ Input Dataë¥¼ í™œìš©í•˜ëŠ”ë°, ì´ë•Œ Encoder ë‹¨ì—ì„œ ì“°ì´ëŠ” Input Dataì™€ êµ¬ë¶„ë˜ëŠ” Input Data ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë¶ˆì–´(ğŸ‡«ğŸ‡·) -> ì˜ì–´(ğŸ‡ºğŸ‡¸) ë²ˆì—­ ëª¨ë¸ì„ ì˜ˆë¡œë“¤ë©´ EnoderëŠ” ë¶ˆì–´ Input Dataë¥¼, DecoderëŠ” ì˜ì–´ Input Dataë¥¼ ì‚¬ìš©í•´ì•¼í•©ë‹ˆë‹¤.

Decoder ë¬¸ë‹¨ì€ Encoderì—ì„œ ì„¤ëª…í•œ êµ¬ì¡°ë¥¼ ìƒëµí•˜ê³  Encoder ì—†ëŠ” êµ¬ì¡°ì™€ íŠ¹ì§•ì„ ìœ„ì£¼ë¡œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

### Masked Mulit-head Attention

<img alt='decoder_block_0' src='./img/decoder_block_0.png'>

MaskëŠ” Encoderì™€ Decoderì—ì„œ í•œ ë²ˆì”© ì‚¬ìš©ë˜ë©° ìˆ˜í–‰ ë°©ë²•ì€ ë™ì¼í•˜ë‚˜ ì‚¬ìš©í•˜ëŠ” ëª©ì ì€ ë‹¤ë¦…ë‹ˆë‹¤. Encoderì—ì„œëŠ” Maskingì„ ëª¨ë¸ì´ Padë¥¼ í•™ìŠµí•˜ì§€ ì•Šê²Œí•˜ê¸° ìœ„í•´ ì‚¬ìš©í–ˆë‹¤ë©´, DecoderëŠ” `Teacher Forcing`ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Maskë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ë¶ˆ ë²ˆì—­ê¸°ë¥¼ ë§Œë“ ë‹¤ê³  ìƒê°í•  ë•Œ Encoder ë‚´ë¶€ì˜ Mask í•¨ìˆ˜ëŠ” ë¶ˆì–´ ë¬¸ì¥ Input ë‚´ ì¡´ì¬í•˜ëŠ” Padì— ëŒ€í•´ Maskingì„ ìˆ˜í–‰í•˜ë©° DecoderëŠ” ì˜ì–´ ë¬¸ì¥ Inputì„ í™œìš©í•´ Teacher Forcing ìˆ˜í–‰ì— í•„ìš”í•œ êµë³¸ì„ ë§Œë“¤ê¸° ìœ„í•´ Maskingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

Teacher Forcingì€ ê¸°ì¡´ì˜ RNN êµ¬ì¡°ì—ì„œ í™œìš©ë˜ëŠ” ê¸°ë²•ì´ì—ˆìŠµë‹ˆë‹¤. TranformerëŠ” RNNìœ¼ë¡œ êµ¬í˜„í•œ Seq2Seq ëª¨ë¸ì„ Attentionìœ¼ë¡œ ìƒˆë¡­ê²Œ ë§Œë“  ê²ƒì´ë¯€ë¡œ ê¸°ì¡´ì˜ RNNì—ì„œ ìˆ˜í–‰í–ˆë˜ í•™ìŠµ ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. RNNì˜ í•™ìŠµì€ ì´ì „ì— ì˜ˆì¸¡í•œ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì—ì¸¡ì„ ìˆ˜í–‰í•˜ë©° í•™ìŠµí•©ë‹ˆë‹¤. `Je suis Ã©tudiant â†’ I am a studient`ì„ í•™ìŠµí•˜ê³ ì ê²½ìš°, ì´ì „ ë¬¸ì¥ì´ `<sos> I ` ì´ë¼ë©´ ì´ ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ì€ ì˜ˆì¸¡ì´ í‹€ë¦´ ê²½ìš° ë‹¤ìŒ ë‹¨ì–´ì— ì˜ˆì¸¡í•  ë‹¨ì–´ì—ë„ ì˜í–¥ì„ ì£¼ê²Œë˜ì–´ ê²°ê³¼ì ìœ¼ë¡œ ì˜ëª»ëœ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ `<sos> I is`ë¼ ì˜ˆì¸¡í–ˆì„ ë•Œ ë°”ë¡œì¡ì§€ ì•Šê³  ê³„ì†í•´ì„œ í•™ìŠµì„ ì´ì–´ë‚˜ê°„ë‹¤ë©´ ì˜ëª»ëœ ë‚´ìš©ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë§‰ê³ ì ì •ë‹µì¸ amì„ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ë”ë¼ë„ ë‹¤ìŒ ë²ˆ ì˜ˆì¸¡ì€ ì›ë˜ ì •ë‹µì¸ `<sos> I am`ì„ í†µí•´ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ RNN ëª¨ë¸ í•™ìŠµ ë°©ë²•ì„ Teacher Forcingì´ë¼ í•˜ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ í‹€ë¦´ì§€ë¼ë„ í•™ìŠµì— ì˜í–¥ì„ ì£¼ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©° ëª¨ë“  ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ê¸°íšŒë¥¼ ì œê³µí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

ë‹¤ì‹œ ë³¸ë¡ ìœ¼ë¡œ ë„˜ì–´ê°€ Maskingì„ Teacher Forcingì— í•„ìš”í•œ êµë³¸ì„ ë§Œë“¤ê¸° ìœ„í•´ í™œìš©í•œë‹¤ëŠ” ë§ì€ ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œìš”?

ì´ëŠ” Transformerê°€ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë¹„ë¡¯í•©ë‹ˆë‹¤. RNNì€ ë¬¸ì¥ í•˜ë‚˜ë¥¼ í•™ìŠµí•  ë•Œ Tokenì„ í•˜ë‚˜ í•˜ë‚˜ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì§„í–‰í–ˆë‹¤ë©´ TransformerëŠ” ë³‘ë ¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œëŠ” Teacher forcingì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ì „ ì˜ˆì¸¡ì€ ë‹¤ìŒ ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ ì˜ˆì¸¡ì´ ë…ë¦½ì ìœ¼ë¡œ ì§„í–‰ëœë‹¤ëŠ” ì˜ë¯¸ì´ë©° ì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì´ ê°œë³„ ë‹¨ì–´ë¥¼ í•™ìŠµ í•  ìˆ˜ ìˆë„ë¡ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë¥¼ ì œê±°í•´ì•¼í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Maskingì„ í™œìš©í•´ í•™ìŠµì— í™œìš©ë  ë°ì´í„°ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìƒì„±í•´ì•¼í•©ë‹ˆë‹¤.

  <img alt='masked_attention' src='./img/masked_attention.png'>

- **ì£¼ì˜! Decoder Masked Mulit-head attentionì—ì„œ ìˆ˜í–‰í•˜ëŠ” Maskingì€ ëª¨ë¸ í•™ìŠµì—ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì‹¤ì „ì—ì„œëŠ” ê°€ë¦´ ë‚´ìš©ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**

```python

    def make_trg_mask(self, trg):
        """
        Masked Multi-head attention pad í•¨ìˆ˜
        """
        # trg = triangle
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )
        return trg_mask.to(self.device)
```

### Multi-head Attentionì—ì„œ Context í™œìš©

<img alt='decoder_block_1' src='./img/decoder_block_1.png'>

Decoderì™€ Encoderì˜ Multi-head Attention êµ¬ì¡°ëŠ” ë™ì¼í•˜ë‚˜ Input Dataì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Encoder ëë‹¨ì—ì„œ ì´ì–´ì§„ í™”ì‚´í‘œê°€ ëª¨ë“  Decoderì˜ Input Dataë¡œ ë“¤ì–´ê°ì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ Masked Multi-head Attentionì—ì„œ í•™ìŠµí•œ ê°’ë„ Multi-head Attentionì— í•¨ê»˜ Input Dataë¡œì„œ ë°›ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰ ë‘ ê°œì˜ ë¬¸ì¥ì´ Decoderì˜ Multi-head Attetnionìœ¼ë¡œ í™œìš©ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Self-Attentionê³¼ Cross-Attention ë¬¸ë‹¨ì—ì„œ ì„¤ëª…í–ˆë“¯ ë‘ ê°œì˜ ë¬¸ì¥ì´ ë“¤ì–´ê°€ë¯€ë¡œ Decoderì˜ Multi-head Attetnionì€ Cross Attentionì…ë‹ˆë‹¤.

<img alt='decoder' src='./img/decoder.png'>

Decoderì˜ Multi-head Attentionì€ Encoderì˜ Contextì™€ Decoderì˜ Masked Mulit-head Attentionì˜ outputì„ ê²°í•©í•˜ëŠ” ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤. Encoderì˜ ê²°ê³¼ê°’ì„ Contextë¼ ë¶€ë¥´ëŠ” ì´ìœ ëŠ” Nê°œì˜ Encoderë¥¼ ê±°ì¹˜ë©´ì„œ ë¬¸ì¥ ë‚´ í† í¬ë“¤ì˜ ì„œë¡œê°„ ê´€ê³„ê°€ ë³µì¡í•˜ê²Œ ë…¹ì•„ë“¤ì–´ê°”ê¸° ë•Œë¬¸ì— ë¬¸ì¥ì˜ ë¬¸ë§¥ì„ ë‹´ì•˜ë‹¤ëŠ” ì˜ë¯¸ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

Decoderì˜ Multi-head Attetnionì€ Contextì—ì„œ Key,Valueë¥¼, Masked Multi-head Attentionì˜ outputì—ì„œ Queryë¥¼ ì¶”ì¶œí•˜ì—¬ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©°, ê°œë³„ queryë¥¼ contextì˜ keyë¡œ Scoreë¥¼ êµ¬í•œ ë’¤ ìµœì¢… Attentionì„ êµ¬í•¨ìœ¼ë¡œì„œ ë¬¸ë§¥ ì •ë³´ë¥¼ í¬í•¨í•œ ë²ˆì—­ì´ ê°€ëŠ¥í•˜ê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ Scoreì„ êµ¬í•˜ê²Œ ë˜ë©´ ì˜ì–´ ë¬¸ì¥ê³¼ ë¶ˆì–´ ë¬¸ì¥ ê°„ ê´€ê³„ íŒŒì•…ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë©° ì´ë¥¼ Softmaxí•œ ë’¤ Valueë¡œ ê³±í•˜ì—¬ íŠ¹ì • ë¶ˆì–´ ë‹¨ì–´ì— ì§‘ì¤‘í•´ì•¼í•  ì˜ì–´ ë‹¨ì–´ë“¤ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img alt='decoder_attention' src='./img/decoder_attention.png'>

### Decoder Block êµ¬í˜„

<img alt='decoder_block_3' src='./img/decoder_block_3.png'>

```python
class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:
        """
        config ì°¸ê³ 
        embed_size(=512) : embedding ì°¨ì›
        heads(=8) : Attention ê°œìˆ˜
        dropout(=0.1): Node í•™ìŠµ ë¹„ìœ¨
        forward_expansion(=2) : FFNNì˜ ì°¨ì›ì„ ì–¼ë§ˆë‚˜ ëŠ˜ë¦´ ê²ƒì¸ì§€ ê²°ì •,
                                forward_expension * embed_size(2*512 = 1024)
        """
        super().__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = selfAttention(embed_size, heads=heads)
        self.encoder_block = EncoderBlock(embed_size, heads, dropout, forward_expansion)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_trg_mask, target_mask):
        """
        x : target input with_embedding (n, trg_token_len, embed_size) 3
        value, key : encoder_attention (n, src_token_len, embed_size) 3
        """

        # masked_attention
        attention = self.attention(x, x, x, target_mask)
        # (n, trg_token_len, embed_size) 3

        # add & Norm
        query = self.dropout(self.norm(attention + x))

        # encoder_decoder attention + feed_forward
        out = self.encoder_block(value, key, query, src_trg_mask)
        # (n, trg_token_len, embed_size) 3

        return out

```

### Decoder êµ¬í˜„

<img alt='decoder_1' src='./img/decoder_1.png'>

```python
class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        max_length,
        device,
    ) -> None:
        """
        config ì°¸ê³ 
        trg_vocab_size(=10873) : input vocab ê°œìˆ˜
        embed_size(=512) : embedding ì°¨ì›
        num_layers(=3) : Encoder Block ê°œìˆ˜
        heads(=8) : Attention ê°œìˆ˜
        forward_expansion(=2) : FFNNì˜ ì°¨ì›ì„ ì–¼ë§ˆë‚˜ ëŠ˜ë¦´ ê²ƒì¸ì§€ ê²°ì •,
                                forward_expension * embed_size(2*512 = 1024)
        dropout(=0.1): Node í•™ìŠµ ë¹„ìœ¨
        max_length : batch ë¬¸ì¥ ë‚´ ìµœëŒ€ token ê°œìˆ˜
        device : cpu
        """
        super().__init__()
        self.device = device

        # ì‹œì‘ë¶€ë¶„ êµ¬í˜„(input + positional_embeding)
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)  # (10837,512) 2

        # positional embedding
        pos_embed = torch.zeros(max_length, embed_size)  # (trg_token_len, embed_size) 2
        pos_embed.requires_grad = False
        position = torch.arange(0, max_length).float().unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size)
        )
        pos_embed[:, 0::2] = torch.sin(position * div_term)
        pos_embed[:, 1::2] = torch.cos(position * div_term)
        self.pos_embed = pos_embed.unsqueeze(0).to(device)
        # (1, trg_token_len, embed_size) 3

        # Decoder Layer êµ¬í˜„
        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, dropout, forward_expansion)
                for _ in range(num_layers)
            ]
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_src, src_trg_mask, trg_mask):
        # n : batch_size(=128)
        # trg_token_len : batch ë‚´ ë¬¸ì¥ ì¤‘ ìµœëŒ€ í† í° ê°œìˆ˜

        _, seq_len = x.size()
        # (n, trg_token_len)

        pos_embed = self.pos_embed[:, :seq_len, :]
        # (1, trg_token_len, embed_size) 3

        out = self.dropout(self.word_embedding(x) + pos_embed).to(self.device)
        # (n, trg_token_len, embed_size) 3

        for layer in self.layers:
            # Decoder Input, Encoder(K), Encoder(V) , src_trg_mask, trg_mask
            out = layer(out, enc_src, enc_src, src_trg_mask, trg_mask)
        return out


```

## Linear FC layer & Softmax

<img alt='output' src='./img/output.png'>

<br/>

ì´ë ‡ê²Œ ì—¬ëŸ¬ë²ˆì˜ Decoderë¥¼ ê±°ì¹œ ê²°ê³¼ì˜ Shapeì€ (N_batch x max_length x embed_size)ê°€ ë©ë‹ˆë‹¤. Decoder ìˆ˜í–‰ì„ ë§ˆì¹œ Outputì„ í™œìš©í•´ ì‹¤ì œ ì˜ì–´ Textë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ë¯€ë¡œ Outputì„ ì˜ì–´ Vocab Size ë§Œí¼ í™•ì¥ì‹œí‚¤ëŠ” ê³¼ì •ì„ ê±°ì³ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ (N_batch x max_length x vocab_size) Shapeìœ¼ë¡œì˜ í™•ì¥ì„ ìœ„í•œ Linear FC Layerê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì˜ì–´ Vocab Sizeë§Œí¼ í™•ì¥ëœ embeddingì— softmaxë¥¼ êµ¬í•˜ë©´ ê°œë³„ ë‹¨ì–´ì˜ í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ê°€ ê³§ ì„ íƒì§€ ì…ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ Seq2Seq ëª¨ë¸ì´ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. í•™ìŠµì´ ëë‚œ ëª¨ë¸ì´ ì‹¤ì œ ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” Decoderì˜ Input Dataë¡œ < sos >ë¥¼ ë„£ê³  ì‹œì‘í•©ë‹ˆë‹¤. ì´ë•Œ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ë¶ˆì–´ ë¬¸ì¥ ë¬¸ë§¥ ì •ë³´ê°€ ë…¹ì•„ìˆëŠ” Contextì™€ Input dataì˜ < sos >ë¥¼ ê°€ì§€ê³  < sos > ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. í•™ìŠµì´ ì˜ëœ ëª¨ë¸ì´ë¼ë©´ `I`ë¥¼ ì‚°ì¶œí•  ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ë„ ë™ì¼í•©ë‹ˆë‹¤. Decoder Input Dataì— `< sos > + I`ë¥¼ ë„£ìœ¼ë©´ Contextì™€ Input dataë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ ë¬¸ì¥ ì¢…ë£Œë¥¼ ì˜ë¯¸í•˜ëŠ” < bos >ê°€ ë‚˜ì˜¬ë•Œ ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì€ Seq2Seq ëª¨ë¸ì´ ì‹¤ì œ Outputì„ ì‚°ì¶œí•˜ëŠ” ê³¼ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<img alt='decoding_process' src='./img/decoding_process.gif'>
<figcaption>Jay Alammar, The Illustrated Transformer</figcaption>

## Transformer ìµœì¢… êµ¬í˜„

<img alt='img11' src='./img/img11.png'>

```python
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size,
        num_layers,
        forward_expansion,
        heads,
        dropout,
        device,
        max_length,
    ) -> None:
        """
        src_vocab_size(=11509) : source vocab ê°œìˆ˜
        trg_vocab_size(=10873) : target vocab ê°œìˆ˜
        src_pad_idx(=1) : source vocabì˜ <pad> idx
        trg_pad_idx(=1) : source vocabì˜ <pad> idx
        embed_size(=512) : embedding ì°¨ì›
        num_layers(=3) : Encoder Block ê°œìˆ˜
        forward_expansion(=2) : FFNNì˜ ì°¨ì›ì„ ì–¼ë§ˆë‚˜ ëŠ˜ë¦´ ê²ƒì¸ì§€ ê²°ì •,
                                forward_expension * embed_size(2*512 = 1024)
        heads(=8) : Attention ê°œìˆ˜
        dropout(=0.1): Node í•™ìŠµ ë¹„ìœ¨
        device : cpu
        max_length(=140) : batch ë¬¸ì¥ ë‚´ ìµœëŒ€ token ê°œìˆ˜
        """
        super().__init__()
        self.Encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            max_length,
            device,
        )
        self.Decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            max_length,
            device,
        )
        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

        # Probability Generlator
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)  # (512,10873) 2

    def encode(self, src):
        """
        Test ìš©ë„ë¡œ í™œìš© encoder ê¸°ëŠ¥
        """
        src_mask = self.make_pad_mask(src, src)
        return self.Encoder(src, src_mask)

    def decode(self, src, trg, enc_src):
        """
        Test ìš©ë„ë¡œ í™œìš© decoder ê¸°ëŠ¥
        """
        # decode
        src_trg_mask = self.make_pad_mask(trg, src)
        trg_mask = self.make_trg_mask(trg)
        out = self.Decoder(trg, enc_src, src_trg_mask, trg_mask)
        # Linear Layer
        out = self.fc_out(out)  # (n, decoder_query_len, trg_vocab_size) 3

        # Softmax
        out = F.log_softmax(out, dim=-1)
        return out

    def make_pad_mask(self, query, key):
        """
        Multi-head attention pad í•¨ìˆ˜
        """
        len_query, len_key = query.size(1), key.size(1)

        key = key.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (batch_size x 1 x 1 x src_token_len) 4

        key = key.repeat(1, 1, len_query, 1)
        # (batch_size x 1 x len_query x src_token_len) 4

        query = query.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)
        # (batch_size x 1 x src_token_len x 1) 4

        query = query.repeat(1, 1, 1, len_key)
        # (batch_size x 1 x src_token_len x src_token_len) 4

        mask = key & query
        return mask

    def make_trg_mask(self, trg):
        """
        Masked Multi-head attention pad í•¨ìˆ˜
        """
        # trg = triangle
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )
        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_pad_mask(src, src)
        # (n,1,src_token_len,src_token_len) 4

        trg_mask = self.make_trg_mask(trg)
        # (n,1,trg_token_len,trg_token_len) 4

        src_trg_mask = self.make_pad_mask(trg, src)
        # (n,1,trg_token_len,src_token_len) 4

        enc_src = self.Encoder(src, src_mask)
        # (n, src_token_len, embed_size) 3

        out = self.Decoder(trg, enc_src, src_trg_mask, trg_mask)
        # (n, trg_token_len, embed_size) 3

        # Linear Layer
        out = self.fc_out(out)  # embed_size => trg_vocab_size
        # (n, trg_token_len, trg_vocab_size) 3

        # Softmax
        out = F.log_softmax(out, dim=-1)
        return out


```

## ì°¸ê³ ìë£Œ

[[hansu kim] [NLP ë…¼ë¬¸ êµ¬í˜„] pytorchë¡œ êµ¬í˜„í•˜ëŠ” Transformer (Attention is All You Need)](https://cpm0722.github.io/pytorch-implementation/transformer)

[[Jay Alammar] The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

[[Aimb] Self-Attentionê³¼ Masked Self-Attention](https://aimb.tistory.com/182)

[[ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€] 08-2: Transformer ](https://www.youtube.com/watch?v=Yk1tV_cXMMU&t=1422s)

[[Aimb] Self-Attentionê³¼ Masked Self-Attention](https://aimb.tistory.com/182)

[[ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸] 16-01 íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)](https://wikidocs.net/31379)
