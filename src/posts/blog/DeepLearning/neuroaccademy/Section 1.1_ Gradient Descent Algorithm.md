---
title: "Section 1.1_ Gradient Descent Algorithm"
category: "DeepLearning"
date: "2022-09-18"
thumbnail: "./img/nuromatch.png"
---

```python
# Imports
import torch
import numpy as np
from torch import nn
from math import pi
import matplotlib.pyplot as plt

import random
import torch

def set_seed(seed=None, seed_torch=True):
  """
  Function that controls randomness. NumPy and random modules must be imported.

  Args:
    seed : Integer
      A non-negative integer that defines the random state. Default is `None`.
    seed_torch : Boolean
      If `True` sets the random seed for pytorch tensors, so pytorch module
      must be imported. Default is `True`.

  Returns:
    Nothing.
  """
  if seed is None:
    seed = np.random.choice(2 ** 32)
  random.seed(seed)
  np.random.seed(seed)
  if seed_torch:
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

  print(f'Random seed {seed} has been set.')


# In case that `DataLoader` is used
def seed_worker(worker_id):
  """
  DataLoader will reseed workers following randomness in
  multi-process data loading algorithm.

  Args:
    worker_id: integer
      ID of subprocess to seed. 0 means that
      the data will be loaded in the main process
      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details

  Returns:
    Nothing
  """
  worker_seed = torch.initial_seed() % 2**32
  np.random.seed(worker_seed)
  random.seed(worker_seed)
```

```python
SEED = 2021
set_seed(seed=SEED)
DEVICE = 'mps'
```

    Random seed 2021 has been set.

## Section 1: Gradient Descent Algorithm

the goal of learning algorithms = minimizing the risk function

> risk = cost = loss

gradient descent = powerful optimization methods

Neural Network function = $y=f_w(x)$

> tuning ê°€ëŠ¥í•œ wë¥¼ í¬í•¨í•œ í•¨ìˆ˜

A loss function = $L=âˆ‚(y,data)$

> nnì˜ ì¶œë ¥ê°’(y)ê³¼ ê²°ê³¼ê°’(data)ì„ ë¹„êµ

Optimization problem : $w^* = argmin_wâˆ‚(f_w(x),data)$

> ìµœì ì˜ wê°’ì€ loss_functionì˜ ìµœì†Ÿê°’ì¸ ê²½ìš°ë¥¼ ë§í•¨. ìœ„ ë‘ í•¨ìˆ˜ë¥¼ í•©ì¹œ ê²ƒ

### íŠ¹ì • functionì˜ GradientëŠ” í•­ìƒ ê°€ì¥ ê°€íŒŒë¥¸ ìƒìŠ¹ ë°©í–¥ì„ ì§€ëª©í•œë‹¤ê³  í•œë‹¤..

**íŠ¹ì • functionì˜ Gradient vector ì°¾ê¸°(ê³µì‹ ìœ ë„)**

- ì˜ˆì‹œ
  $\begin{equation}
z = h(x, y) = \sin(x^2 + y^2)
\end{equation}$

- gradient vector ì°¾ê¸°
  $\begin{equation}
  \begin{bmatrix}
  \dfrac{\partial z}{\partial x} \\ \\ \dfrac{\partial z}{\partial y}
  \end{bmatrix}
\end{equation}$

**Chain Rule ì´í•´í•˜ê¸°**

- ê¸°ë³¸ ë£°
  $\begin{equation}F(x) = g(h(x)) \equiv (g \circ h)(x)\end{equation}$

- ë¯¸ë¶„ í•  ê²½ìš°
  $\begin{equation}
F'(x) = g'(h(x)) \cdot h'(x)
\end{equation}$

- ê²°ê³¼
  ![as](./img/W1D2_Tutorial1_Solution_115a15ba_0.png)

**ê°„ëµí•œ gradient descentì˜ ì—­ì‚¬**

In 1847, Augustin-Louis Cauchy used negative of gradients to develop the Gradient Descent algorithm as an iterative method to minimize a continuous and (ideally) differentiable function of many variables.

<br>

### ìµœì¢… ê²°ë¡  : gradientëŠ” ìµœê³  ë†’ì€ ìœ„ì¹˜ë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì¸ê±°ê³  ê·¸ gradientë¥¼ negative í•œ ì‹ì´ gradient descentì´ë‹¤.

### Gradient Descent ê·¸ë˜í”„ ì´í•´í•˜ê¸°

![a](./img/gradient_1.png)

- gradientë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ $\theta_0$ ì™€ $\theta_1$ë¥¼ ê²©ìë¡œ ìƒì„±í•´ì„œ lossë¥¼ í•˜ë‚˜í•˜ë‚˜ êµ¬í•´ì•¼í•¨.

- ì´ëŸ°ì‹ìœ¼ë¡œ ì¼ì¼ì´ êµ¬í•œ ë’¤ ìµœì†Œì˜ loss ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” $\theta_0$ ì™€ $\theta_1$ë¥¼ ì°¾ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ ì ì„

<br/>

![a](./img/gradient_2.png)

- gradientë¼ëŠ” ê°œë… ìì²´ê°€ ë§¤ ì—°ì‚° ë‹¤ìŒ ë‹¨ê³„ ì¤‘ ìµœì ì˜ ë°©í–¥ì„ ë¯¸ë¦¬ ì•Œë ¤ì¤Œ

- ë”°ë¼ì„œ ê²€ì€ìƒ‰ ì„ ê³¼ ê°™ì´, ê°€ì¥ íš¨ìœ¨ì ì¸ ê°’ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” gradient descentë¥¼ ì‚¬ìš©í•´ì•¼í•¨.

## Section 1.2: Gradient Descent Algorithm

$\begin{equation}
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla f \left( \mathbf{w}^{(t)} \right)
\end{equation}$

- $\eta$ = learning Rate

$\nabla f (\mathbf{w})= \left( \frac{\partial f(\mathbf{w})}{\partial w_1}, ..., \frac{\partial f(\mathbf{w})}{\partial w_d} \right)$ ì„ ì•Œë©´ ë‹¤ìŒ w ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

### ê²°ë¡ 

Since negative gradients always point locally in the direction of steepest descent, the algorithm makes small steps at each point towards the minimum.

### ìµœì ì˜ í•´ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ psudo codeë¡œ í‘œí˜„í•˜ë©´??

> Inputs: initial guess ğ°(0), step size ğœ‚>0, number of steps ğ‘‡.

> For ğ‘¡=0,1,2,â€¦,ğ‘‡âˆ’1 do
>
> &nbsp; **ğ°(ğ‘¡+1)=ğ°(ğ‘¡)âˆ’ğœ‚âˆ‡ğ‘“(ğ°(ğ‘¡))**
>
> end

> Return: ğ°(ğ‘¡+1)

**Computational Graphë¥¼ í™œìš©í•´ $\nabla f (\mathbf{w})$ êµ¬í•´ë³´ê¸°**

**ì˜ˆì‹œ í•¨ìˆ˜**

$\begin{equation}
f(x, y, z) = \tanh \left(\ln \left[1 + z \frac{2x}{sin(y)} \right] \right)
\end{equation}$

**$\nabla f (\mathbf{w})$ ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•**

$\begin{equation}
\dfrac{\partial Loss}{\partial \mathbf{w}} = \left[ \dfrac{\partial Loss}{\partial w_1}, \dfrac{\partial Loss}{\partial w_2} , \dots, \dfrac{\partial Loss}{\partial w_d} \right]^{\top}
\end{equation}$

<br/>

- ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ê°’ë“¤(1~d) : $\dfrac{\partial Loss}{\partial w_d}$

- $\dfrac{\partial Loss}{\partial w_d}$ì„ í•˜ë‚˜ì”© êµ¬í•´ë³´ì.

### Computational Graph(forward) - ì›ë˜ ê°œë…

- ìœ„ ê³µì‹ì˜ ê³„ì‚° ê³¼ì •ì„ í•˜ë‚˜í•˜ë‚˜ ëœ¯ì–´ë‚´ ë„ì‹í™” í•˜ì˜€ìŒ

- x,y,zë¥¼ ëŒ€ì…í•´ $f$ë¥¼ ì¶œë ¥ í•¨

  ![a](./img/comput_graph_forward.png)

<br/>

### Computational Graph(backward) - ë¸íƒ€ ê°’ êµ¬í•˜ê¸° ìœ„í•´ ì‘ìš©

- $f$ ê°’ì„ ê°€ì§€ê³  $\dfrac{\partial f}{\partial x}$, $\dfrac{\partial f}{\partial y}$, $\dfrac{\partial f}{\partial z}$ ê°’ì„ ì¶œë ¥í•¨

- íŒŒë€ìƒ‰ ë„¤ëª¨ ë°•ìŠ¤ëŠ” ë¯¸ë¶„í•œ ê°’

  ![a](./img/comput_graph_backward.png)

### ê²°ë¡ 

$\begin{equation}
\dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial e}~\dfrac{\partial e}{\partial d}~\dfrac{\partial d}{\partial c}~\dfrac{\partial c}{\partial a}~\dfrac{\partial a}{\partial x} = \left( 1-\tanh^2(e) \right) \cdot \frac{1}{d+1}\cdot z \cdot \frac{1}{b} \cdot 2
\end{equation}$
